{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Congrats, you finished the part on the data preparation, and we can now move on to a more exciting part of using RNNs/LSTMs to process sequential data! But be careful, even if the previous notebook might seem a little bit boring, it is of great importance. We switched from images to text data in this exercise, and remember the first steps that we did in our class were also data related, and they were essential for all the following exercises. So naturally, since we switched to text data in this exercise, make sure you have a good understanding of how the data has been prepared.\n",
    "\n",
    "For the last I2DL exercise, we want to make use of Recurrent Neural Networks (RNNs) to process sequential data. We will stick with the same dataset we have been looking at in the previous notebook, namely the [IMDb](https://ai.stanford.edu/~amaas/data/sentiment/) sentiment analysis dataset that contains positive and negative movie reviews.\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"images/IMDB.jpg\" alt=\"centered image\" />\n",
    "</p>\n",
    "\n",
    "Sentiment analysis is the task of predicting the sentiment of a text. In this notebook, you will train a network to process reviews from the dataset and evaluate whether it has been a positive or a negative review. Below are two examples:\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"images/examples.png\" alt=\"centered image\" />\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0\n",
    "# !python -m pip install pytorch-lightning==1.6.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "As always, we first import some packages to setup the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from exercise_code.rnn.sentiment_dataset import (\n",
    "    download_data,\n",
    "    load_sentiment_data,\n",
    "    load_vocab,\n",
    "    SentimentDataset,\n",
    "    collate\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data\n",
    "\n",
    "As we have learned from the notebook 1, this time we not only load the raw data, but also have the corresponding vocabulary. Let us load the data that we prepared for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 9154\n",
      "number of validation samples: 3133\n",
      "number of test samples: 3083\n"
     ]
    }
   ],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"SentimentData\")\n",
    "base_dir = download_data(data_root)\n",
    "vocab = load_vocab(base_dir)\n",
    "train_data, val_data, test_data = load_sentiment_data(base_dir, vocab)\n",
    "\n",
    "print(\"number of training samples: {}\".format(len(train_data)))\n",
    "print(\"number of validation samples: {}\".format(len(val_data)))\n",
    "print(\"number of test samples: {}\".format(len(test_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Samples\n",
    "\n",
    "Our raw data consists of tuples `(raw_text, token_list, token_indices, label)`. Let's sample some relatively short texts from our dataset to have a sense how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      " You'd better choose Paul Verhoeven's even if you have watched it.\n",
      "\n",
      "Tokens: \n",
      " ['you', 'd', 'better', 'choose', 'paul', 'verhoeven', 's', 'even', 'if', 'you', 'have', 'watched', 'it']\n",
      "\n",
      "Indices: \n",
      " [20, 232, 107, 1999, 855, 4624, 16, 64, 35, 20, 26, 214, 8]\n",
      "\n",
      "Label:\n",
      " 0\n",
      "\n",
      "\n",
      "Text: \n",
      " This movie is terrible but it has some good effects.\n",
      "\n",
      "Tokens: \n",
      " ['this', 'movie', 'is', 'terrible', 'but', 'it', 'has', 'some', 'good', 'effects']\n",
      "\n",
      "Indices: \n",
      " [10, 13, 9, 270, 17, 8, 53, 52, 33, 264]\n",
      "\n",
      "Label:\n",
      " 0\n",
      "\n",
      "\n",
      "Text: \n",
      " I don't know why I like this movie so well, but I never get tired of watching it.\n",
      "\n",
      "Tokens: \n",
      " ['i', 'don', 't', 'know', 'why', 'i', 'like', 'this', 'movie', 'so', 'well', 'but', 'i', 'never', 'get', 'tired', 'of', 'watching', 'it']\n",
      "\n",
      "Indices: \n",
      " [7, 74, 23, 126, 138, 7, 32, 10, 13, 34, 68, 17, 7, 115, 82, 1225, 5, 116, 8]\n",
      "\n",
      "Label:\n",
      " 1\n",
      "\n",
      "\n",
      "Text: \n",
      " Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "Tokens: \n",
      " ['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville']\n",
      "\n",
      "Indices: \n",
      " [1, 340, 1308, 9, 2, 91, 340, 5, 1, 8, 16, 47, 352, 340, 5, 1]\n",
      "\n",
      "Label:\n",
      " 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data0 = [datum for datum in train_data if len(datum[1]) < 20 and datum[-1] == 0] # negative\n",
    "sample_data1 = [datum for datum in train_data if len(datum[1]) < 20 and datum[-1] == 1] # positive\n",
    "\n",
    "# we sample 2 tuples each from positive set and negative set\n",
    "sample_data = random.sample(sample_data0, 2) + random.sample(sample_data1, 2)\n",
    "for text, tokens, indices, label in sample_data:\n",
    "    print('Text: \\n {}\\n'.format(text))\n",
    "    print('Tokens: \\n {}\\n'.format(tokens))\n",
    "    print('Indices: \\n {}\\n'.format(indices))\n",
    "    print('Label:\\n {}\\n'.format(label))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Vocabulary\n",
    "\n",
    "In the previous notebook, we discussed the need of a vocabulary for mapping words to unique integer IDs. Instead of creating the vocabulary manually, we provide you with the vocabulary. Let's have a look at some samples from the vocabulary of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5002 \n",
      "\n",
      "  Sample words\n",
      "--------------------\n",
      " picture\n",
      " im\n",
      " u\n",
      " hunt\n",
      " effectively\n",
      " well\n",
      " island\n",
      " fisher\n",
      " chosen\n",
      " appearances\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(vocab), '\\n\\n  Sample words\\n{}'.format('-' * 20))\n",
    "sample_words = random.sample(list(vocab.keys()), 10)\n",
    "for word in sample_words:\n",
    "    print(' {}'.format(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we saw that there are already indices in the raw data that we loaded. We can check if the indices in the vocabulary match the raw data using the last sentence in `sample_data`. Words that are not in the vocabulary are assigned to the symbol `<unk>`. The output of the following cell should be the same as the indices in the last example of our loaded raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      " Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "\n",
      "Tokens: \n",
      " ['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville']\n",
      "\n",
      "Indices: \n",
      " [1, 340, 1308, 9, 2, 91, 340, 5, 1, 8, 16, 47, 352, 340, 5, 1]\n",
      "\n",
      "Label:\n",
      " 1\n",
      "\n",
      "Indices drawn from vocabulary: \n",
      " [1, 340, 1308, 9, 2, 91, 340, 5, 1, 8, 16, 47, 352, 340, 5, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last sample from above \n",
    "(text, tokens, indices, label) = sample_data[-1]\n",
    "print('Text: \\n {}\\n'.format(text))\n",
    "print('Tokens: \\n {}\\n'.format(tokens))\n",
    "print('Indices: \\n {}\\n'.format(indices))\n",
    "print('Label:\\n {}\\n'.format(label))\n",
    "\n",
    "# Compare with the vocabulary\n",
    "print('Indices drawn from vocabulary: \\n {}\\n'.format([vocab.get(x, vocab['<unk>']) for x in sample_data[-1][1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping to PyTorch Datasets\n",
    "\n",
    "Great, the raw data is loaded properly and the vocabulary is matching. Let us wrap our data in a PyTorch dataset. For more details, check out the previous notebook and the corresponding dataset class defined in `exercise_code/rnn/sentiment_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset Class for train, val and test set\n",
    "train_dataset = SentimentDataset(train_data)\n",
    "val_dataset = SentimentDataset(val_data)\n",
    "test_dataset = SentimentDataset(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a Sentiment Classifier\n",
    "\n",
    "After we have loaded the data, it is time to define a model and start training and testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Since we just need to predict positive or negative, we can use `binary cross-entropy loss` to train our model. And accuracy can be used to assess our model's performance. We will use the following evaluation model to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, data_loader):\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        pred = model(input, lengths)\n",
    "        corrects += ((pred > 0.5) == label).sum().item()\n",
    "        total += label.numel()\n",
    "        \n",
    "        if i > 0  and i % 100 == 0:\n",
    "            print('Step {} / {}'.format(i, len(data_loader)))\n",
    "    \n",
    "    return corrects / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "In this part, you need to create a classifier using the Embedding layers you implemented in the first notebook and LSTM. For the LSTM, you may also use the PyTorch implementation.\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"images/LSTM.png\" alt=\"centered image\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement a Classifier</h3>\n",
    "    \n",
    "   Go to <code>exercise_code/rnn/text_classifiers.py</code> and implement the <code>RNNClassifier</code>. In the skeleton code, we inherited <code>nn.Module</code>. You can also inherit <code>LightningModule</code> if you want to use PyTorch Lightning.\n",
    "</div>\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from `torch.nn.Module` or `pytorch_lightning.LightningModule`\n",
    "* Perform the forward pass in `forward()`.\n",
    "* Have less than 2 million parameters\n",
    "* Have a model size of less than 50MB after saving\n",
    "\n",
    "After you finished, edit the below cell to make sure your implementation is correct. You should define the model yourself, which should be small enough (2 Mio. parameters) and have correct output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1807105\n",
      "Your model is sufficiently small :)\n",
      "(tensor([[[ 0.1772, -0.0030, -0.0175,  ...,  0.1353,  0.1459,  0.1643],\n",
      "         [ 0.0539,  0.1812, -0.0987,  ..., -0.1696, -0.0965,  0.0914],\n",
      "         [ 0.0516,  0.1224, -0.2111,  ..., -0.1324, -0.0516, -0.1286]],\n",
      "\n",
      "        [[ 0.1233,  0.0928, -0.0409,  ...,  0.2763,  0.0045,  0.1666],\n",
      "         [-0.1003,  0.1147, -0.1272,  ..., -0.0991, -0.1116,  0.0089],\n",
      "         [ 0.0938,  0.2631, -0.0165,  ...,  0.0451, -0.0726, -0.0200]],\n",
      "\n",
      "        [[ 0.0977,  0.0976, -0.1538,  ...,  0.1217, -0.0644,  0.1592],\n",
      "         [-0.0858,  0.0395,  0.1421,  ..., -0.1521, -0.0932,  0.1047],\n",
      "         [ 0.0137,  0.1780, -0.1056,  ...,  0.1066, -0.0932, -0.0739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0888, -0.0585, -0.2366,  ..., -0.1940,  0.0435, -0.1709],\n",
      "         [ 0.1518,  0.2188, -0.1322,  ..., -0.0572,  0.0911,  0.0628],\n",
      "         [-0.0055, -0.0293, -0.0205,  ...,  0.2341, -0.0705,  0.0527]],\n",
      "\n",
      "        [[-0.0066,  0.0038, -0.0483,  ..., -0.0594,  0.1270, -0.1004],\n",
      "         [ 0.0824,  0.0562, -0.0170,  ...,  0.1234, -0.0767,  0.0816],\n",
      "         [-0.1847,  0.0133,  0.1280,  ..., -0.0210, -0.0761, -0.1288]],\n",
      "\n",
      "        [[-0.1035, -0.0160,  0.0281,  ..., -0.0777,  0.0550, -0.0600],\n",
      "         [ 0.1179, -0.1424,  0.0938,  ..., -0.1457, -0.2249,  0.0302],\n",
      "         [-0.3135, -0.0098,  0.0523,  ...,  0.0166, -0.0559, -0.0233]]],\n",
      "       grad_fn=<StackBackward0>), (tensor([[[-1.0350e-01, -1.6034e-02,  2.8122e-02, -1.8325e-02,  2.3790e-01,\n",
      "           4.3031e-02,  1.8411e-02, -5.0801e-02,  1.3055e-01, -6.3131e-02,\n",
      "           6.7453e-02,  5.9371e-02,  1.4911e-01, -2.2793e-01,  5.7851e-02,\n",
      "          -1.0279e-01, -1.5936e-01,  1.1123e-01,  1.8692e-02, -3.4645e-02,\n",
      "           5.1580e-02,  2.2363e-01, -2.7622e-02,  1.1587e-01, -1.5806e-01,\n",
      "           1.6889e-01, -4.2855e-02,  1.6434e-02, -2.0959e-01, -1.8929e-01,\n",
      "           1.9799e-01,  1.4755e-01, -1.2166e-01,  4.0538e-02,  5.2644e-02,\n",
      "           1.7087e-01,  2.5952e-01,  1.8656e-02, -8.3799e-02, -2.3422e-02,\n",
      "          -3.4853e-04,  1.3201e-01,  3.3242e-02,  5.5901e-02,  2.1833e-01,\n",
      "           1.7916e-01,  1.6410e-01, -2.3803e-02, -4.1723e-02,  7.0927e-02,\n",
      "          -1.0883e-01,  1.5670e-01, -1.2340e-01, -1.4682e-01,  4.8165e-02,\n",
      "           2.8679e-01,  5.0911e-02, -3.5721e-01,  6.2374e-04, -1.3237e-02,\n",
      "          -1.2429e-01, -1.1694e-01, -3.0828e-02, -1.5222e-01,  4.8957e-02,\n",
      "          -9.3080e-02,  2.0189e-01,  1.6854e-01,  1.1730e-01,  5.5127e-02,\n",
      "          -2.0660e-02,  1.7010e-01,  2.4232e-02,  1.4064e-02, -1.7917e-02,\n",
      "           1.7411e-01,  8.3744e-02,  2.2516e-01, -1.4824e-01,  4.1049e-02,\n",
      "           1.0172e-01,  9.0689e-02,  5.8674e-02, -2.1258e-01, -1.1074e-01,\n",
      "           2.0143e-01, -1.7162e-01, -3.9319e-02,  1.4156e-01, -1.6870e-01,\n",
      "           3.7583e-01, -4.3432e-02, -1.1666e-01, -1.0779e-01, -2.8401e-01,\n",
      "          -3.0816e-01,  2.7313e-01, -7.4010e-02, -5.4276e-02, -1.1928e-01,\n",
      "           4.3603e-02, -7.1930e-02, -2.7624e-01,  8.7755e-02,  8.7431e-02,\n",
      "           1.9529e-01,  2.2817e-01, -1.0347e-01, -9.0390e-03,  3.9333e-02,\n",
      "          -1.4451e-01, -2.3187e-02, -1.4143e-01, -1.4927e-02, -3.5430e-01,\n",
      "           1.4252e-01,  6.3867e-02,  8.9375e-02,  8.8929e-02, -1.7010e-01,\n",
      "          -6.4698e-02,  3.0194e-02,  8.0792e-03, -2.3439e-01, -6.4244e-02,\n",
      "           2.4263e-01,  1.3398e-01, -3.2969e-02,  1.0408e-01,  6.4239e-02,\n",
      "          -1.4487e-01, -2.2330e-01,  7.8249e-02,  3.1752e-01,  2.3070e-01,\n",
      "          -3.9203e-02,  9.4702e-02, -8.8557e-02,  2.2314e-01,  1.5419e-01,\n",
      "          -1.1132e-01,  2.4215e-02,  1.4055e-01, -3.7196e-02,  1.0809e-02,\n",
      "          -8.4072e-02,  9.3972e-02, -9.0882e-02, -1.4849e-01,  1.0084e-01,\n",
      "          -8.5263e-04,  3.4429e-02, -5.2846e-02, -1.7037e-01,  1.8789e-01,\n",
      "          -5.9357e-02,  1.5746e-02, -2.3366e-01, -1.5807e-01,  9.5178e-02,\n",
      "           1.6516e-01, -2.1302e-01, -1.3769e-01, -6.4045e-02, -1.3557e-01,\n",
      "          -3.1246e-02, -3.4470e-02,  1.2257e-01, -7.3063e-02,  4.1710e-02,\n",
      "          -2.1955e-01,  7.1316e-02,  8.8944e-02, -2.2196e-02, -1.9711e-01,\n",
      "          -5.5013e-02, -7.9094e-02, -2.6078e-01,  6.6477e-02, -5.9758e-02,\n",
      "           4.9942e-03,  2.7293e-01,  9.9934e-03, -1.7192e-02,  9.5103e-03,\n",
      "          -1.0415e-02,  1.5063e-01,  3.9423e-03, -2.5347e-01, -6.9253e-02,\n",
      "           9.6336e-02,  5.8478e-02, -2.2049e-01, -1.3453e-01, -3.3551e-02,\n",
      "           7.6300e-03, -2.5498e-03,  1.9828e-01, -2.2724e-01,  1.0243e-01,\n",
      "           4.9353e-02,  1.3062e-01, -1.0231e-02,  1.1317e-01,  1.8400e-01,\n",
      "          -4.0209e-01,  2.2878e-02, -1.3901e-01, -2.8087e-01,  2.7705e-01,\n",
      "          -8.1692e-02, -2.7200e-01,  2.2633e-02, -9.7501e-02,  9.6411e-02,\n",
      "          -1.7014e-01, -4.0242e-02,  5.8845e-02, -6.7816e-02,  1.9617e-01,\n",
      "           1.4886e-01,  4.7680e-02,  7.3001e-02, -5.6526e-02,  1.3770e-01,\n",
      "          -1.0867e-01, -2.0852e-01,  4.2697e-02, -8.7231e-02,  6.5406e-02,\n",
      "           2.2227e-01, -8.5916e-02, -5.1824e-02,  4.2150e-01, -2.3031e-01,\n",
      "          -7.0769e-02,  2.2283e-01, -4.0743e-02, -2.1551e-01,  1.0560e-01,\n",
      "          -2.4911e-02,  4.9446e-02, -1.8352e-02,  2.5352e-01, -1.5141e-01,\n",
      "           8.9892e-02,  6.5212e-02, -3.4348e-02, -4.0323e-02, -1.9325e-01,\n",
      "           5.3401e-02,  2.5423e-01, -1.0113e-01, -7.7651e-02,  5.4962e-02,\n",
      "          -6.0011e-02],\n",
      "         [ 1.1792e-01, -1.4237e-01,  9.3750e-02, -3.3378e-01,  9.2571e-02,\n",
      "           6.9514e-03,  2.4125e-01, -2.6482e-01,  1.6563e-01, -1.4795e-01,\n",
      "          -6.2067e-02, -4.4721e-02, -1.7442e-01, -2.0653e-01,  5.9591e-03,\n",
      "           1.3415e-01,  1.2242e-01,  1.2551e-01,  1.1312e-01, -4.2632e-02,\n",
      "           1.6798e-01, -6.9309e-03,  7.6332e-02,  2.2904e-01,  6.6428e-03,\n",
      "          -8.5625e-03,  3.3163e-03,  5.5478e-03, -1.6146e-01,  1.1377e-01,\n",
      "           9.8923e-02, -2.0678e-01,  2.0650e-01,  1.5577e-01,  1.9079e-01,\n",
      "           2.3786e-01,  2.3952e-01,  3.4723e-02,  2.1873e-01, -6.2999e-02,\n",
      "           1.7869e-01, -2.1652e-03, -1.5745e-01,  2.7141e-02, -9.9253e-02,\n",
      "           1.5047e-01, -5.1642e-03, -7.8325e-02, -5.3468e-02, -1.1627e-01,\n",
      "           8.6034e-02, -3.1409e-03,  1.1082e-01,  1.2794e-01, -5.8825e-03,\n",
      "           7.6169e-02, -3.4109e-01, -2.9456e-01, -1.2450e-01, -1.7884e-01,\n",
      "           1.9995e-01, -8.6735e-03, -7.9656e-02, -4.3643e-02,  2.5969e-02,\n",
      "          -2.0781e-01, -1.1980e-01,  1.2677e-01,  1.5817e-01, -1.0583e-01,\n",
      "           3.2890e-02,  3.8635e-01, -5.8412e-02,  8.1429e-02, -1.2436e-01,\n",
      "          -3.6594e-02,  3.8651e-01, -2.6483e-01, -1.9266e-01,  9.6281e-02,\n",
      "           4.9570e-02,  1.4498e-01, -7.4231e-02,  3.0529e-02,  1.1263e-01,\n",
      "           7.2075e-02, -3.6316e-01, -3.8047e-01,  1.8421e-01,  1.2613e-01,\n",
      "           3.5719e-01, -1.8705e-01,  8.1981e-03, -8.4756e-02,  4.0582e-02,\n",
      "          -1.0289e-01,  5.1409e-02,  5.4810e-02, -2.3936e-01, -5.8729e-02,\n",
      "          -1.9973e-01,  5.6495e-02, -9.7530e-02,  1.7978e-01,  2.0531e-01,\n",
      "          -8.0342e-02, -2.4129e-01, -6.0180e-02,  3.1596e-02, -4.8864e-02,\n",
      "          -6.4566e-02,  9.0024e-02,  1.8372e-01, -1.2010e-01,  8.8934e-02,\n",
      "           4.0745e-02, -2.2882e-01, -2.6174e-01, -1.2285e-02,  1.0287e-01,\n",
      "           1.3665e-01, -2.7068e-02, -7.4024e-03, -6.8822e-02,  2.9088e-01,\n",
      "          -1.1032e-01,  2.1822e-01, -7.7902e-02, -4.2832e-02, -8.6829e-02,\n",
      "           1.6622e-02,  4.2870e-02,  9.5299e-02,  6.3807e-02, -1.0726e-01,\n",
      "           1.6156e-01,  5.2729e-02,  6.1259e-02,  7.2931e-02, -3.3919e-01,\n",
      "          -1.8412e-01,  6.7854e-02,  6.0697e-02, -9.2650e-03,  3.0319e-02,\n",
      "          -1.7956e-01, -1.0197e-01, -9.6657e-02, -7.0556e-02,  1.8566e-01,\n",
      "           5.4147e-02, -9.1208e-02, -1.4730e-01, -6.4205e-02, -3.2037e-03,\n",
      "           1.0112e-02, -4.7742e-02,  7.3584e-02,  1.2840e-01, -1.3579e-02,\n",
      "           6.9094e-02, -3.8812e-02,  3.0997e-02,  9.3722e-03,  2.3788e-01,\n",
      "          -2.1208e-01, -5.1379e-03, -2.5467e-02,  4.6080e-02,  2.9951e-01,\n",
      "           1.7949e-01,  1.6234e-01,  7.5026e-03,  1.5177e-01,  7.0188e-02,\n",
      "          -7.1766e-02,  5.6465e-02, -1.5538e-01, -2.0158e-01,  1.0036e-01,\n",
      "           7.0670e-02,  1.0370e-01, -6.1342e-02, -1.9154e-01, -1.0050e-01,\n",
      "          -2.3503e-01,  1.5783e-01,  2.9772e-01,  2.4905e-01, -3.9065e-02,\n",
      "           8.9153e-02, -5.8257e-02,  1.5996e-01, -1.2242e-01,  1.5368e-01,\n",
      "          -4.4913e-02,  5.5281e-02,  2.0309e-01,  4.6041e-02,  3.5951e-01,\n",
      "          -4.3467e-02, -1.9593e-01, -4.9962e-02, -1.4902e-01, -7.7377e-03,\n",
      "          -3.9314e-02,  5.8737e-02,  1.9468e-01, -5.2022e-02, -9.0010e-02,\n",
      "           1.4182e-01,  4.0647e-02,  8.7830e-02, -3.1040e-01,  8.3115e-02,\n",
      "          -7.9794e-02,  7.5428e-02, -1.3221e-01, -1.9156e-01, -5.9173e-02,\n",
      "           1.5388e-01,  1.1251e-01, -3.2985e-03, -6.2600e-03, -2.1595e-01,\n",
      "          -1.1402e-01, -1.0705e-01,  6.0211e-02,  3.3860e-01,  1.0297e-01,\n",
      "          -7.1583e-02,  1.3007e-01, -6.3493e-02, -7.8072e-02,  8.0431e-02,\n",
      "           8.1257e-02, -1.2757e-01,  2.0170e-02, -3.3500e-02, -2.0555e-01,\n",
      "           3.3910e-02, -1.3141e-01, -1.6467e-01,  1.2233e-01,  1.3370e-02,\n",
      "           1.0356e-01,  3.2047e-01,  3.7298e-01, -1.5015e-01,  1.7683e-01,\n",
      "          -6.5434e-02,  1.2305e-02, -9.1064e-02, -1.4573e-01, -2.2487e-01,\n",
      "           3.0180e-02],\n",
      "         [-3.1354e-01, -9.7650e-03,  5.2324e-02, -4.6986e-02,  5.3338e-02,\n",
      "           1.2709e-01,  1.2639e-02,  1.1030e-03, -3.0639e-01,  1.6899e-01,\n",
      "          -3.0500e-02, -2.8676e-02,  1.7132e-01,  2.7675e-01, -3.0162e-02,\n",
      "          -8.1454e-02,  4.7982e-02,  9.8321e-03, -1.2482e-01, -4.1846e-03,\n",
      "          -3.7279e-02,  9.5057e-02, -2.0628e-02, -7.1136e-02, -5.5735e-03,\n",
      "           7.9446e-02, -1.3970e-02, -7.7616e-02, -4.8020e-02,  5.5293e-02,\n",
      "           6.4030e-02, -5.4032e-02, -6.3764e-02, -1.3746e-01, -2.9137e-02,\n",
      "          -6.0938e-02,  1.6618e-01,  3.9859e-02,  8.7765e-02, -2.3719e-02,\n",
      "           1.6644e-01,  8.1025e-02,  1.2624e-01, -4.3570e-01,  7.0042e-02,\n",
      "           6.1494e-02,  1.2088e-01, -2.4707e-02, -2.7324e-01, -1.3102e-01,\n",
      "          -2.3380e-01, -3.5791e-01, -1.0934e-02, -1.1054e-01, -1.8743e-01,\n",
      "           1.5055e-02, -1.1036e-01,  1.4441e-01, -1.4229e-01, -1.0809e-01,\n",
      "           3.1362e-02, -1.3462e-01,  1.8521e-01, -3.1615e-01, -7.2498e-02,\n",
      "           3.8107e-02,  7.2761e-02,  1.4560e-01,  2.2396e-02, -1.7359e-01,\n",
      "          -1.0389e-01, -1.1875e-01, -1.3213e-02, -2.1111e-02,  1.1916e-01,\n",
      "           3.4238e-02, -2.0965e-01, -8.1743e-02,  6.2087e-03, -8.6046e-02,\n",
      "          -9.1556e-02, -1.0955e-01,  2.0325e-02, -1.5471e-01, -3.9904e-02,\n",
      "           8.5226e-02, -9.1864e-03, -3.0587e-02,  8.2084e-02,  7.0521e-02,\n",
      "          -3.4393e-01,  1.3171e-01, -5.0133e-02,  1.5037e-02,  8.4652e-02,\n",
      "           6.6115e-03,  1.8218e-02,  4.1111e-02,  3.8585e-02, -2.7755e-02,\n",
      "           7.5371e-02,  1.2439e-01,  1.7254e-01, -6.2646e-02, -2.6092e-01,\n",
      "          -2.7753e-01, -7.0655e-02,  3.9838e-02, -1.5333e-01, -6.3210e-02,\n",
      "           3.4051e-02, -3.8061e-02,  1.2118e-01, -1.5009e-01,  1.6887e-01,\n",
      "           1.8185e-01,  1.2479e-02,  6.5831e-02,  6.9456e-03,  2.4189e-02,\n",
      "           2.0525e-01, -9.1310e-02,  1.8611e-02, -2.7313e-02, -1.9994e-01,\n",
      "          -1.1361e-01, -1.9022e-02,  9.4800e-02, -9.7853e-02, -6.5073e-02,\n",
      "           9.1061e-02, -1.6477e-01,  6.3723e-02, -3.4434e-02,  2.5083e-02,\n",
      "          -1.5074e-01,  8.4057e-02,  1.1227e-01, -1.6435e-01, -1.9308e-01,\n",
      "           3.6430e-02,  4.4866e-02, -2.0044e-01, -2.2277e-02, -2.1767e-02,\n",
      "          -2.9543e-01, -8.0444e-02, -6.7816e-02,  1.2452e-01, -1.0323e-01,\n",
      "          -7.5849e-02, -2.2103e-01, -1.3800e-01,  5.7227e-02,  6.4027e-03,\n",
      "          -8.6975e-02,  5.1669e-03,  1.3146e-01,  2.0450e-01, -9.0235e-02,\n",
      "          -1.6544e-01, -7.5750e-02, -2.8410e-02,  3.9695e-02,  7.5713e-02,\n",
      "          -9.2646e-02, -6.0415e-02, -1.1958e-01,  4.3170e-02,  2.0512e-01,\n",
      "          -1.2211e-01,  8.1265e-02, -1.7897e-01, -1.3665e-01, -1.0247e-01,\n",
      "           1.3458e-01,  1.0320e-01,  3.8837e-01,  1.6200e-01, -6.8193e-02,\n",
      "          -1.2102e-01, -1.2272e-03, -1.3810e-01, -1.0544e-01, -7.4249e-02,\n",
      "           3.2085e-02, -9.0719e-02,  1.6320e-01, -5.5187e-03,  9.0069e-02,\n",
      "           1.3200e-01, -1.5979e-01, -6.7487e-02,  1.8422e-01,  3.9586e-02,\n",
      "           7.5853e-02,  6.3325e-02,  1.0656e-01, -1.4683e-02, -4.0771e-02,\n",
      "           2.0466e-01,  9.0344e-02, -9.6452e-02,  9.1897e-02, -1.7194e-01,\n",
      "          -1.8329e-02, -1.1693e-01, -7.7680e-03, -1.3121e-01, -2.5370e-01,\n",
      "          -8.1858e-03, -1.4423e-01,  5.6608e-02, -1.6233e-01,  1.5378e-01,\n",
      "           1.7690e-01,  5.0532e-02, -1.8202e-01,  1.0873e-01,  1.5996e-02,\n",
      "           2.8110e-02, -8.2468e-02, -4.2909e-02,  4.3183e-02,  3.1667e-02,\n",
      "          -1.7138e-01, -6.1825e-02, -2.1608e-01,  2.5268e-02, -7.5461e-02,\n",
      "           5.2338e-02,  1.6672e-01, -2.1054e-02, -1.2631e-01,  7.6813e-02,\n",
      "          -8.2906e-02,  1.5645e-02, -7.5809e-02,  2.8499e-02, -2.4251e-02,\n",
      "           1.5150e-03, -3.2841e-02,  1.6676e-01,  2.1883e-01, -7.1145e-02,\n",
      "           1.4163e-02,  2.1952e-01, -7.0417e-02,  8.9132e-02, -2.4592e-02,\n",
      "           7.5564e-02,  2.1272e-01,  1.5446e-01,  1.6603e-02, -5.5924e-02,\n",
      "          -2.3346e-02]]], grad_fn=<StackBackward0>), tensor([[[-0.2378, -0.0686,  0.0504, -0.0386,  0.4264,  0.0833,  0.0982,\n",
      "          -0.2152,  0.2842, -0.0915,  0.1192,  0.1679,  0.2215, -0.3277,\n",
      "           0.1604, -0.1487, -0.3572,  0.2746,  0.0438, -0.0961,  0.1699,\n",
      "           0.6211, -0.0542,  0.1776, -0.4394,  0.3717, -0.0959,  0.0891,\n",
      "          -0.4313, -0.5618,  0.4027,  0.2785, -0.2535,  0.0840,  0.1270,\n",
      "           0.3027,  0.7668,  0.0322, -0.2195, -0.0436, -0.0012,  0.2164,\n",
      "           0.0704,  0.1454,  0.4501,  0.6147,  0.2265, -0.0530, -0.0747,\n",
      "           0.1120, -0.1657,  0.3025, -0.3325, -0.3433,  0.1258,  0.4463,\n",
      "           0.0996, -0.6229,  0.0018, -0.0356, -0.2726, -0.2417, -0.0918,\n",
      "          -0.4502,  0.0853, -0.1656,  0.2970,  0.2218,  0.2707,  0.1433,\n",
      "          -0.0417,  0.3122,  0.0585,  0.0283, -0.0339,  0.2874,  0.1564,\n",
      "           0.5010, -0.2773,  0.0578,  0.1477,  0.2291,  0.1401, -0.3924,\n",
      "          -0.2388,  0.3224, -0.3023, -0.0951,  0.1753, -0.2752,  0.7025,\n",
      "          -0.1467, -0.3427, -0.1466, -0.3794, -0.5147,  0.6037, -0.1632,\n",
      "          -0.2019, -0.2621,  0.0792, -0.1555, -0.6201,  0.1968,  0.2395,\n",
      "           0.3290,  0.3209, -0.1289, -0.0145,  0.0753, -0.3917, -0.0347,\n",
      "          -0.1964, -0.0207, -0.6257,  0.1745,  0.2779,  0.1662,  0.1325,\n",
      "          -0.4260, -0.1690,  0.0456,  0.0269, -0.4796, -0.3417,  0.5181,\n",
      "           0.2586, -0.0768,  0.2275,  0.1822, -0.2056, -0.3315,  0.1315,\n",
      "           0.4776,  0.5130, -0.0749,  0.2943, -0.1481,  0.4000,  0.2555,\n",
      "          -0.1629,  0.1340,  0.2801, -0.0720,  0.0179, -0.1649,  0.1585,\n",
      "          -0.1970, -0.2683,  0.1857, -0.0015,  0.0548, -0.1165, -0.2884,\n",
      "           0.3706, -0.2180,  0.0587, -0.3699, -0.2207,  0.2692,  0.3420,\n",
      "          -0.5798, -0.2572, -0.3127, -0.2968, -0.0522, -0.0697,  0.1842,\n",
      "          -0.1787,  0.1028, -0.3503,  0.1981,  0.2490, -0.0686, -0.3385,\n",
      "          -0.0743, -0.1536, -0.4746,  0.1151, -0.1358,  0.0134,  0.4935,\n",
      "           0.0255, -0.0435,  0.0146, -0.0171,  0.4015,  0.0139, -0.4152,\n",
      "          -0.1732,  0.1747,  0.1138, -0.5506, -0.4061, -0.0596,  0.0135,\n",
      "          -0.0058,  0.4120, -0.4948,  0.2071,  0.1364,  0.3497, -0.0270,\n",
      "           0.2400,  0.3586, -0.5924,  0.0396, -0.2836, -0.4379,  0.5849,\n",
      "          -0.1250, -0.4920,  0.0408, -0.2045,  0.1314, -0.4093, -0.0511,\n",
      "           0.1014, -0.1091,  0.3685,  0.2861,  0.1049,  0.2294, -0.0970,\n",
      "           0.3303, -0.1756, -0.4666,  0.0764, -0.2008,  0.1641,  0.5250,\n",
      "          -0.1371, -0.1441,  0.9853, -0.4288, -0.1299,  0.3128, -0.0875,\n",
      "          -0.5849,  0.2142, -0.0483,  0.2239, -0.0437,  0.3949, -0.2786,\n",
      "           0.2344,  0.1405, -0.0525, -0.1127, -0.3531,  0.0838,  0.5194,\n",
      "          -0.2211, -0.1347,  0.1460, -0.1113],\n",
      "         [ 0.2299, -0.2326,  0.1978, -0.7282,  0.1711,  0.0162,  0.3797,\n",
      "          -0.4220,  0.2454, -0.2249, -0.1213, -0.1341, -0.2459, -0.5375,\n",
      "           0.0124,  0.2071,  0.2793,  0.2681,  0.1544, -0.0697,  0.4802,\n",
      "          -0.0099,  0.1255,  0.3869,  0.0123, -0.0319,  0.0078,  0.0212,\n",
      "          -0.2300,  0.1554,  0.2004, -0.2565,  0.4675,  0.4107,  0.3474,\n",
      "           0.3727,  0.5408,  0.0906,  0.5116, -0.1003,  0.5361, -0.0048,\n",
      "          -0.2479,  0.0961, -0.2669,  0.2064, -0.0133, -0.1347, -0.1022,\n",
      "          -0.3057,  0.1523, -0.0068,  0.2280,  0.1926, -0.0111,  0.1184,\n",
      "          -0.5990, -0.6066, -0.1842, -0.5209,  0.4417, -0.0147, -0.2013,\n",
      "          -0.1444,  0.0512, -0.2681, -0.2081,  0.2154,  0.3492, -0.2074,\n",
      "           0.0577,  0.6528, -0.1045,  0.1586, -0.2108, -0.1146,  0.7187,\n",
      "          -0.3977, -0.3755,  0.1863,  0.1269,  0.2639, -0.1609,  0.1410,\n",
      "           0.4476,  0.2476, -0.8336, -0.7272,  0.3431,  0.3298,  0.6722,\n",
      "          -0.3226,  0.0155, -0.2388,  0.0616, -0.5590,  0.0981,  0.1055,\n",
      "          -0.4179, -0.1435, -0.2986,  0.1558, -0.1445,  0.2514,  0.4091,\n",
      "          -0.1558, -0.4052, -0.1308,  0.0732, -0.0967, -0.1175,  0.1547,\n",
      "           0.3947, -0.2328,  0.2897,  0.1345, -0.3393, -0.5367, -0.0232,\n",
      "           0.2264,  0.3631, -0.0488, -0.0136, -0.1006,  0.6226, -0.1876,\n",
      "           0.2993, -0.3100, -0.0918, -0.1496,  0.0214,  0.1574,  0.1624,\n",
      "           0.0963, -0.2772,  0.4793,  0.0890,  0.2344,  0.1285, -0.5880,\n",
      "          -0.2420,  0.2630,  0.2676, -0.0235,  0.0548, -0.3483, -0.3477,\n",
      "          -0.3135, -0.2959,  0.3105,  0.1099, -0.1882, -0.4017, -0.1462,\n",
      "          -0.0058,  0.0156, -0.1516,  0.1687,  0.2315, -0.0308,  0.1339,\n",
      "          -0.0593,  0.0537,  0.0159,  0.4404, -0.3523, -0.0113, -0.0495,\n",
      "           0.0923,  0.4154,  0.4279,  0.3038,  0.0098,  0.4996,  0.1558,\n",
      "          -0.2046,  0.1902, -0.2408, -0.3631,  0.1915,  0.1919,  0.2506,\n",
      "          -0.1017, -0.2286, -0.2297, -0.5625,  0.5388,  0.6000,  0.5686,\n",
      "          -0.1066,  0.2132, -0.2070,  0.3344, -0.2545,  0.2171, -0.0795,\n",
      "           0.1237,  0.3254,  0.1412,  0.4931, -0.1245, -0.3828, -0.1320,\n",
      "          -0.2293, -0.0210, -0.0729,  0.2168,  0.3439, -0.0662, -0.1843,\n",
      "           0.2802,  0.1849,  0.1568, -0.7346,  0.1878, -0.1415,  0.1258,\n",
      "          -0.6913, -0.2592, -0.2227,  0.2402,  0.2570, -0.0068, -0.0158,\n",
      "          -0.4346, -0.2509, -0.2412,  0.1581,  0.5350,  0.2855, -0.1670,\n",
      "           0.3033, -0.1355, -0.1472,  0.3152,  0.1657, -0.2588,  0.0614,\n",
      "          -0.0539, -0.4027,  0.0636, -0.2069, -0.5294,  0.2395,  0.0268,\n",
      "           0.1844,  0.5965,  0.6310, -0.4090,  0.2650, -0.1111,  0.0397,\n",
      "          -0.2058, -0.3293, -0.4912,  0.0793],\n",
      "         [-0.8140, -0.0181,  0.1451, -0.0911,  0.0970,  0.4603,  0.0345,\n",
      "           0.0065, -0.5712,  0.3319, -0.0676, -0.0572,  0.2820,  0.4881,\n",
      "          -0.0543, -0.2644,  0.1329,  0.0150, -0.2481, -0.0078, -0.0864,\n",
      "           0.3543, -0.0476, -0.1554, -0.0136,  0.1384, -0.0224, -0.1983,\n",
      "          -0.1169,  0.1001,  0.1025, -0.0866, -0.1138, -0.2094, -0.1608,\n",
      "          -0.1304,  0.5385,  0.0696,  0.4294, -0.0442,  0.2937,  0.3776,\n",
      "           0.4901, -0.6606,  0.2012,  0.0884,  0.1479, -0.0461, -0.4727,\n",
      "          -0.3041, -0.4344, -0.6199, -0.0179, -0.3910, -0.4300,  0.0240,\n",
      "          -0.1355,  0.2995, -0.2898, -0.2270,  0.0776, -0.2696,  0.3570,\n",
      "          -0.5343, -0.1496,  0.0846,  0.2351,  0.2400,  0.0546, -0.3341,\n",
      "          -0.1666, -0.2107, -0.0307, -0.0365,  0.3270,  0.0921, -0.4318,\n",
      "          -0.1635,  0.0129, -0.1183, -0.1986, -0.3211,  0.0541, -0.3174,\n",
      "          -0.1093,  0.1412, -0.0148, -0.0798,  0.1341,  0.1202, -0.5952,\n",
      "           0.2743, -0.1725,  0.0407,  0.1050,  0.0091,  0.0362,  0.1285,\n",
      "           0.1158, -0.0460,  0.2313,  0.1959,  0.2486, -0.1106, -0.5913,\n",
      "          -0.5372, -0.1287,  0.1117, -0.3767, -0.1983,  0.0889, -0.0686,\n",
      "           0.1808, -0.3689,  0.5532,  0.2997,  0.0597,  0.0919,  0.0138,\n",
      "           0.0591,  0.4131, -0.2430,  0.0293, -0.0415, -0.4734, -0.1656,\n",
      "          -0.0494,  0.1492, -0.1412, -0.1201,  0.1912, -0.2036,  0.1156,\n",
      "          -0.0585,  0.0541, -0.2114,  0.3542,  0.3064, -0.3265, -0.5895,\n",
      "           0.0537,  0.1323, -0.3091, -0.0493, -0.0351, -0.5370, -0.1528,\n",
      "          -0.3039,  0.1819, -0.2940, -0.1342, -0.3144, -0.4264,  0.1669,\n",
      "           0.0143, -0.3427,  0.0150,  0.2343,  0.3511, -0.1652, -0.2952,\n",
      "          -0.1521, -0.0797,  0.1390,  0.1645, -0.1408, -0.0812, -0.2752,\n",
      "           0.1365,  0.6586, -0.2621,  0.2435, -0.3387, -0.2312, -0.1618,\n",
      "           0.3709,  0.1694,  0.8479,  0.2452, -0.1235, -0.2186, -0.0028,\n",
      "          -0.3573, -0.1823, -0.1726,  0.0952, -0.1372,  0.2463, -0.0088,\n",
      "           0.1716,  0.2657, -0.2426, -0.2039,  0.3396,  0.0669,  0.1096,\n",
      "           0.1087,  0.4625, -0.0369, -0.0889,  0.3784,  0.1471, -0.1358,\n",
      "           0.2550, -0.2419, -0.0386, -0.5352, -0.0489, -0.1852, -0.5171,\n",
      "          -0.0164, -0.5030,  0.0950, -0.3725,  0.4067,  0.2873,  0.1060,\n",
      "          -0.3195,  0.3257,  0.0274,  0.0444, -0.1754, -0.1276,  0.1045,\n",
      "           0.0625, -0.3591, -0.2181, -0.4662,  0.0593, -0.1772,  0.2521,\n",
      "           0.2695, -0.0485, -0.2083,  0.1633, -0.1661,  0.0242, -0.1458,\n",
      "           0.0566, -0.0537,  0.0030, -0.0729,  0.2930,  0.3310, -0.1328,\n",
      "           0.0446,  0.4862, -0.2166,  0.2678, -0.0800,  0.1492,  0.3507,\n",
      "           0.5492,  0.0292, -0.1822, -0.0471]]], grad_fn=<StackBackward0>)))\n",
      "(PackedSequence(data=tensor([[ 0.1772, -0.0030, -0.0175,  ...,  0.1353,  0.1459,  0.1643],\n",
      "        [ 0.0539,  0.1812, -0.0987,  ..., -0.1696, -0.0965,  0.0914],\n",
      "        [ 0.0516,  0.1224, -0.2111,  ..., -0.1324, -0.0516, -0.1286],\n",
      "        ...,\n",
      "        [-0.0066,  0.0038, -0.0483,  ..., -0.0594,  0.1270, -0.1004],\n",
      "        [ 0.0824,  0.0562, -0.0170,  ...,  0.1234, -0.0767,  0.0816],\n",
      "        [-0.1035, -0.0160,  0.0281,  ..., -0.0777,  0.0550, -0.0600]],\n",
      "       grad_fn=<CatBackward0>), batch_sizes=tensor([3, 3, 3, 3, 3, 3, 3, 3, 2, 1]), sorted_indices=None, unsorted_indices=None), (tensor([[[-1.0350e-01, -1.6034e-02,  2.8122e-02, -1.8325e-02,  2.3790e-01,\n",
      "           4.3031e-02,  1.8411e-02, -5.0801e-02,  1.3055e-01, -6.3131e-02,\n",
      "           6.7453e-02,  5.9371e-02,  1.4911e-01, -2.2793e-01,  5.7851e-02,\n",
      "          -1.0279e-01, -1.5936e-01,  1.1123e-01,  1.8692e-02, -3.4645e-02,\n",
      "           5.1580e-02,  2.2363e-01, -2.7622e-02,  1.1587e-01, -1.5806e-01,\n",
      "           1.6889e-01, -4.2855e-02,  1.6434e-02, -2.0959e-01, -1.8929e-01,\n",
      "           1.9799e-01,  1.4755e-01, -1.2166e-01,  4.0538e-02,  5.2644e-02,\n",
      "           1.7087e-01,  2.5952e-01,  1.8656e-02, -8.3799e-02, -2.3422e-02,\n",
      "          -3.4854e-04,  1.3201e-01,  3.3242e-02,  5.5901e-02,  2.1833e-01,\n",
      "           1.7916e-01,  1.6410e-01, -2.3803e-02, -4.1723e-02,  7.0927e-02,\n",
      "          -1.0883e-01,  1.5670e-01, -1.2340e-01, -1.4682e-01,  4.8165e-02,\n",
      "           2.8679e-01,  5.0911e-02, -3.5721e-01,  6.2374e-04, -1.3237e-02,\n",
      "          -1.2429e-01, -1.1694e-01, -3.0828e-02, -1.5222e-01,  4.8957e-02,\n",
      "          -9.3080e-02,  2.0189e-01,  1.6854e-01,  1.1730e-01,  5.5127e-02,\n",
      "          -2.0660e-02,  1.7010e-01,  2.4232e-02,  1.4064e-02, -1.7917e-02,\n",
      "           1.7411e-01,  8.3744e-02,  2.2516e-01, -1.4824e-01,  4.1049e-02,\n",
      "           1.0172e-01,  9.0689e-02,  5.8674e-02, -2.1258e-01, -1.1074e-01,\n",
      "           2.0143e-01, -1.7162e-01, -3.9319e-02,  1.4156e-01, -1.6870e-01,\n",
      "           3.7583e-01, -4.3432e-02, -1.1666e-01, -1.0779e-01, -2.8401e-01,\n",
      "          -3.0816e-01,  2.7313e-01, -7.4010e-02, -5.4276e-02, -1.1928e-01,\n",
      "           4.3603e-02, -7.1930e-02, -2.7624e-01,  8.7755e-02,  8.7431e-02,\n",
      "           1.9529e-01,  2.2817e-01, -1.0347e-01, -9.0389e-03,  3.9333e-02,\n",
      "          -1.4451e-01, -2.3187e-02, -1.4143e-01, -1.4927e-02, -3.5430e-01,\n",
      "           1.4252e-01,  6.3867e-02,  8.9375e-02,  8.8929e-02, -1.7010e-01,\n",
      "          -6.4698e-02,  3.0194e-02,  8.0792e-03, -2.3439e-01, -6.4244e-02,\n",
      "           2.4263e-01,  1.3398e-01, -3.2969e-02,  1.0408e-01,  6.4239e-02,\n",
      "          -1.4487e-01, -2.2330e-01,  7.8249e-02,  3.1752e-01,  2.3070e-01,\n",
      "          -3.9203e-02,  9.4702e-02, -8.8557e-02,  2.2314e-01,  1.5419e-01,\n",
      "          -1.1132e-01,  2.4215e-02,  1.4055e-01, -3.7196e-02,  1.0809e-02,\n",
      "          -8.4072e-02,  9.3972e-02, -9.0882e-02, -1.4849e-01,  1.0084e-01,\n",
      "          -8.5263e-04,  3.4429e-02, -5.2846e-02, -1.7037e-01,  1.8789e-01,\n",
      "          -5.9357e-02,  1.5746e-02, -2.3366e-01, -1.5807e-01,  9.5178e-02,\n",
      "           1.6516e-01, -2.1302e-01, -1.3769e-01, -6.4045e-02, -1.3557e-01,\n",
      "          -3.1246e-02, -3.4470e-02,  1.2257e-01, -7.3063e-02,  4.1710e-02,\n",
      "          -2.1955e-01,  7.1316e-02,  8.8944e-02, -2.2196e-02, -1.9711e-01,\n",
      "          -5.5013e-02, -7.9094e-02, -2.6078e-01,  6.6477e-02, -5.9758e-02,\n",
      "           4.9942e-03,  2.7293e-01,  9.9934e-03, -1.7192e-02,  9.5103e-03,\n",
      "          -1.0415e-02,  1.5063e-01,  3.9423e-03, -2.5347e-01, -6.9253e-02,\n",
      "           9.6336e-02,  5.8478e-02, -2.2049e-01, -1.3453e-01, -3.3551e-02,\n",
      "           7.6300e-03, -2.5498e-03,  1.9828e-01, -2.2724e-01,  1.0243e-01,\n",
      "           4.9353e-02,  1.3062e-01, -1.0231e-02,  1.1317e-01,  1.8400e-01,\n",
      "          -4.0209e-01,  2.2878e-02, -1.3901e-01, -2.8087e-01,  2.7705e-01,\n",
      "          -8.1692e-02, -2.7200e-01,  2.2633e-02, -9.7501e-02,  9.6411e-02,\n",
      "          -1.7014e-01, -4.0242e-02,  5.8845e-02, -6.7816e-02,  1.9617e-01,\n",
      "           1.4886e-01,  4.7680e-02,  7.3001e-02, -5.6526e-02,  1.3770e-01,\n",
      "          -1.0867e-01, -2.0852e-01,  4.2697e-02, -8.7231e-02,  6.5406e-02,\n",
      "           2.2227e-01, -8.5916e-02, -5.1824e-02,  4.2150e-01, -2.3031e-01,\n",
      "          -7.0769e-02,  2.2283e-01, -4.0743e-02, -2.1551e-01,  1.0560e-01,\n",
      "          -2.4911e-02,  4.9446e-02, -1.8352e-02,  2.5352e-01, -1.5141e-01,\n",
      "           8.9892e-02,  6.5212e-02, -3.4348e-02, -4.0323e-02, -1.9325e-01,\n",
      "           5.3401e-02,  2.5423e-01, -1.0113e-01, -7.7651e-02,  5.4962e-02,\n",
      "          -6.0011e-02],\n",
      "         [ 8.2380e-02,  5.6236e-02, -1.6978e-02, -2.0613e-01, -1.5042e-01,\n",
      "           7.5943e-02,  1.7931e-01, -2.0284e-01,  2.7597e-02, -6.9575e-02,\n",
      "          -1.2091e-01, -6.1278e-02,  4.7351e-02, -2.4725e-01,  8.3263e-02,\n",
      "           1.8144e-03,  4.5152e-03, -7.6690e-02, -3.9731e-02, -8.5539e-02,\n",
      "           8.9643e-02,  1.4870e-01, -1.0016e-01,  7.5317e-02, -1.2116e-01,\n",
      "           9.0705e-02, -1.1005e-01, -4.6713e-02, -2.8852e-01,  5.5540e-02,\n",
      "          -1.7926e-01, -6.3153e-02, -1.2184e-02,  2.5758e-01, -3.6815e-02,\n",
      "           1.5881e-01,  1.1596e-01,  2.6103e-02,  1.0049e-01,  1.2698e-02,\n",
      "           5.2065e-02,  1.2039e-01,  3.6756e-02, -3.9329e-02,  1.8367e-02,\n",
      "           1.7305e-02,  4.3386e-03, -8.6704e-02, -1.2285e-01, -1.4828e-02,\n",
      "           1.8015e-02,  3.4386e-02, -1.2935e-01, -7.6295e-02, -3.0293e-02,\n",
      "           7.6780e-02, -2.8127e-01, -1.9773e-01, -2.2863e-01, -6.3713e-02,\n",
      "           2.2126e-01, -5.9425e-02, -7.4302e-02, -2.4938e-01, -2.6622e-01,\n",
      "          -1.9004e-01, -1.1080e-01, -2.0567e-02,  1.9319e-02, -6.8757e-02,\n",
      "           1.0785e-01,  3.1022e-01, -1.0854e-01,  1.0862e-01, -1.2707e-01,\n",
      "           3.5694e-02,  2.1695e-01,  4.7600e-02, -1.9831e-01, -1.6710e-02,\n",
      "           3.1770e-02,  4.0839e-02, -2.6247e-01, -3.8620e-02, -1.0405e-01,\n",
      "           7.3510e-02, -4.3954e-01, -1.9466e-01,  9.2177e-02,  4.4458e-02,\n",
      "           2.9704e-01, -1.0646e-01,  2.2875e-02, -8.5085e-03, -4.5137e-02,\n",
      "          -1.4659e-01, -7.0674e-02,  8.8241e-02, -1.2232e-01, -1.7758e-01,\n",
      "          -2.8216e-01,  5.7079e-02, -7.4046e-02,  2.4425e-02,  8.1327e-02,\n",
      "          -6.8425e-02, -1.8085e-01,  2.5025e-02, -3.8607e-02,  6.7103e-02,\n",
      "          -1.4123e-01,  4.9904e-02,  1.3853e-01,  4.6858e-02, -1.2279e-01,\n",
      "           9.5017e-02, -1.2950e-01, -1.4960e-01, -6.0111e-02,  1.3699e-01,\n",
      "           1.1986e-02, -5.5964e-02,  1.9111e-01,  6.5926e-02,  5.2334e-02,\n",
      "          -1.3290e-01,  1.4622e-02,  1.8944e-02, -2.0031e-01, -1.7456e-01,\n",
      "           4.3896e-02,  7.9042e-02, -5.1015e-02,  1.0016e-01,  1.0685e-01,\n",
      "          -4.0499e-02,  2.0715e-01, -1.4986e-02,  1.1255e-01,  1.9179e-03,\n",
      "          -1.8012e-02,  1.8505e-01,  2.0456e-01, -8.3108e-02, -1.5445e-01,\n",
      "          -2.4870e-01, -4.8880e-02, -1.8724e-01, -2.2621e-01, -2.6847e-01,\n",
      "           5.0654e-02, -3.9690e-02,  1.1888e-02,  1.1625e-01,  6.3046e-03,\n",
      "           1.7280e-02, -9.0818e-03,  4.1876e-02,  2.4886e-01, -2.0385e-01,\n",
      "          -1.0413e-01, -4.8376e-02,  9.1541e-02, -7.4596e-02,  1.1175e-01,\n",
      "          -1.1924e-01,  1.1101e-01,  2.6247e-02,  5.0739e-02,  2.4657e-01,\n",
      "           1.2634e-01,  7.6482e-02,  1.3672e-01,  1.2386e-01,  1.1921e-02,\n",
      "          -7.7332e-02,  2.2010e-01, -2.0231e-01, -8.1660e-02,  4.6254e-02,\n",
      "           1.2848e-01,  8.2334e-02,  1.8371e-01, -7.5769e-03,  9.1284e-02,\n",
      "          -1.1987e-01,  6.7494e-02, -3.7922e-02,  7.9430e-02, -1.1080e-01,\n",
      "           1.9207e-02, -1.7593e-01,  1.7154e-01, -5.4879e-02, -1.3521e-01,\n",
      "          -1.0642e-01, -1.1214e-01,  8.8302e-02, -9.6175e-02,  6.6250e-02,\n",
      "           6.1357e-02, -9.8303e-02,  3.4204e-02, -2.0348e-01, -1.6999e-01,\n",
      "          -5.5066e-02,  1.6980e-01,  1.0334e-01, -2.1222e-01,  7.3844e-02,\n",
      "           1.4787e-01,  2.8339e-02,  1.0648e-01, -1.1252e-01,  5.1097e-02,\n",
      "          -3.0244e-01,  8.6528e-02, -1.6927e-01, -2.8568e-01, -6.7716e-02,\n",
      "          -7.6425e-02,  1.8040e-01, -2.8741e-02,  1.5114e-01, -2.4561e-01,\n",
      "          -1.1635e-01, -1.5083e-01,  7.1037e-02,  3.6528e-01, -1.6525e-01,\n",
      "          -4.8283e-02,  1.8853e-01, -3.5140e-02,  1.4986e-01,  4.8798e-03,\n",
      "          -1.9235e-02, -1.8668e-01,  7.4507e-02,  4.7803e-02, -1.2307e-01,\n",
      "           1.2306e-01,  1.9338e-02, -2.6627e-01,  1.5436e-01, -3.3068e-01,\n",
      "           1.1824e-01,  1.0174e-01,  1.4723e-01, -9.4608e-02,  1.1665e-01,\n",
      "          -1.8797e-01,  2.9981e-01,  1.1018e-01,  1.2341e-01, -7.6742e-02,\n",
      "           8.1650e-02],\n",
      "         [-5.4766e-03, -2.9348e-02, -2.0501e-02,  7.5290e-02,  1.1249e-02,\n",
      "           1.3997e-01,  9.7628e-02, -1.7135e-01, -6.2987e-02,  1.1974e-02,\n",
      "          -6.4031e-02,  5.1139e-02, -1.4093e-01,  7.6619e-02,  1.0905e-01,\n",
      "          -7.0577e-02,  2.6331e-01, -5.4092e-02,  6.4151e-02, -5.6715e-02,\n",
      "          -2.4037e-02,  1.1593e-01,  7.8072e-02,  5.5732e-02, -1.0887e-01,\n",
      "          -1.1113e-01, -2.3001e-02, -5.1974e-02,  9.2985e-02, -9.1241e-02,\n",
      "          -1.0044e-01,  1.5403e-01, -1.7531e-02, -1.5376e-01,  1.6917e-02,\n",
      "          -9.2023e-02, -2.7469e-02,  2.7029e-02, -4.0278e-02, -7.1002e-02,\n",
      "           3.7681e-01, -4.0407e-02, -1.1337e-01, -2.0606e-01, -1.0457e-01,\n",
      "          -1.6208e-01, -2.5086e-02,  1.7343e-01, -2.4930e-01, -5.7014e-02,\n",
      "          -1.7717e-01,  1.1812e-02, -3.8477e-02,  4.2216e-02,  6.8680e-02,\n",
      "          -2.7279e-02, -2.7003e-01,  4.1015e-02,  1.9575e-02,  4.2510e-02,\n",
      "           2.2629e-01,  6.1417e-02,  1.5973e-01, -2.9842e-01, -1.7943e-01,\n",
      "           8.1687e-02,  1.9035e-02,  7.8868e-02,  3.6865e-02,  8.6455e-02,\n",
      "          -6.3255e-03, -8.7792e-02, -9.1061e-02,  2.5862e-03,  1.0850e-01,\n",
      "           3.3753e-01, -3.0185e-02, -5.5217e-02, -2.4311e-02,  1.6114e-01,\n",
      "           7.1003e-02, -1.4746e-01, -5.1675e-02,  6.8334e-02, -2.4132e-02,\n",
      "          -6.2866e-02, -3.6700e-02,  5.7953e-02, -2.8571e-02, -8.2932e-02,\n",
      "          -1.2847e-02, -8.0935e-02,  4.4750e-02, -1.4223e-01, -2.3920e-02,\n",
      "           3.8175e-01,  1.7231e-01, -9.6725e-02,  4.8779e-03,  1.9880e-01,\n",
      "           7.6118e-02,  1.1036e-01, -5.0364e-02, -6.0028e-02, -7.1777e-02,\n",
      "          -2.8211e-01,  1.2760e-01, -8.6309e-02, -1.2579e-01,  1.6127e-01,\n",
      "           7.2702e-02,  3.8440e-03, -1.0938e-01,  3.1618e-03, -3.3373e-02,\n",
      "          -1.1090e-01, -1.9423e-01,  9.9716e-02,  1.6206e-01,  2.0976e-01,\n",
      "           4.5164e-02, -1.1121e-01,  7.3949e-02, -1.3071e-02, -1.0635e-01,\n",
      "          -7.9306e-02,  2.1608e-02,  5.6841e-02, -1.1633e-01,  8.7143e-02,\n",
      "           1.1353e-01,  2.9508e-01,  1.3931e-02, -1.2118e-01,  6.7138e-02,\n",
      "           2.4448e-01,  1.9314e-01, -2.0216e-02, -3.9345e-01, -7.6266e-02,\n",
      "          -2.0296e-01,  7.7053e-03, -2.5784e-02, -2.4600e-01, -8.0141e-02,\n",
      "          -4.0310e-01, -1.2400e-01, -1.5131e-02,  1.3390e-01, -9.8666e-02,\n",
      "           1.5799e-01,  1.1184e-01, -1.4761e-01,  2.0908e-01, -2.0853e-01,\n",
      "          -4.6504e-02,  1.2175e-01,  1.4382e-01,  9.9400e-02,  4.1305e-02,\n",
      "          -1.0404e-01,  9.6231e-03,  1.5579e-01, -1.0086e-01, -2.0197e-01,\n",
      "           1.5157e-01, -9.1780e-02, -2.1103e-01,  1.7931e-01,  3.1047e-02,\n",
      "          -2.9312e-02,  6.4530e-03, -1.1185e-01,  6.5745e-03, -1.5281e-01,\n",
      "          -7.5032e-02,  3.4352e-02,  1.0838e-01, -5.0164e-02, -4.9111e-02,\n",
      "          -3.0448e-01,  1.0602e-01, -3.1601e-02,  7.4398e-02,  5.3114e-02,\n",
      "           3.4621e-02,  3.7570e-02, -7.5268e-02,  2.5500e-02,  7.0789e-02,\n",
      "           9.0492e-02,  8.1733e-02, -2.1109e-02,  1.6372e-01,  7.7044e-02,\n",
      "           3.2524e-02,  3.6084e-02, -3.5508e-02,  1.0014e-01,  2.0270e-01,\n",
      "          -3.5425e-02,  2.4983e-02,  2.6339e-02, -1.4010e-01, -1.8122e-01,\n",
      "          -1.0565e-02,  6.4028e-02,  7.2683e-02,  2.1904e-01, -1.7530e-01,\n",
      "           9.1294e-02,  3.7342e-03, -5.7675e-02,  2.0316e-02,  1.7132e-01,\n",
      "          -1.0042e-01,  3.4001e-02, -2.1611e-01,  4.1575e-02, -6.9217e-02,\n",
      "          -1.1886e-01, -1.2090e-01, -1.6614e-01, -3.5312e-01,  1.6640e-01,\n",
      "          -5.1730e-02,  9.8370e-02,  2.0606e-02, -7.4383e-02, -1.1797e-01,\n",
      "           6.9869e-02,  7.9225e-02,  4.8719e-02,  1.0481e-01, -8.1307e-02,\n",
      "          -4.8719e-02, -1.4976e-02,  1.6727e-01, -2.3833e-02, -3.7488e-02,\n",
      "           5.8450e-02,  1.8159e-01,  1.5992e-02, -1.2871e-01, -5.4652e-02,\n",
      "           2.8022e-02,  6.4213e-02, -1.8527e-01, -4.3176e-02,  4.1484e-01,\n",
      "           1.3942e-01,  1.0015e-01,  3.2142e-03,  2.3410e-01, -7.0451e-02,\n",
      "           5.2671e-02]]], grad_fn=<StackBackward0>), tensor([[[-0.2378, -0.0686,  0.0504, -0.0386,  0.4264,  0.0833,  0.0982,\n",
      "          -0.2152,  0.2842, -0.0915,  0.1192,  0.1679,  0.2215, -0.3277,\n",
      "           0.1604, -0.1487, -0.3572,  0.2746,  0.0438, -0.0961,  0.1699,\n",
      "           0.6211, -0.0542,  0.1776, -0.4394,  0.3717, -0.0959,  0.0891,\n",
      "          -0.4313, -0.5618,  0.4027,  0.2785, -0.2535,  0.0840,  0.1270,\n",
      "           0.3027,  0.7668,  0.0322, -0.2195, -0.0436, -0.0012,  0.2164,\n",
      "           0.0704,  0.1454,  0.4501,  0.6147,  0.2265, -0.0530, -0.0747,\n",
      "           0.1120, -0.1657,  0.3025, -0.3325, -0.3433,  0.1258,  0.4463,\n",
      "           0.0996, -0.6229,  0.0018, -0.0356, -0.2726, -0.2417, -0.0918,\n",
      "          -0.4502,  0.0853, -0.1656,  0.2970,  0.2218,  0.2707,  0.1433,\n",
      "          -0.0417,  0.3122,  0.0585,  0.0283, -0.0339,  0.2874,  0.1564,\n",
      "           0.5010, -0.2773,  0.0578,  0.1477,  0.2291,  0.1401, -0.3924,\n",
      "          -0.2388,  0.3224, -0.3023, -0.0951,  0.1753, -0.2752,  0.7025,\n",
      "          -0.1467, -0.3427, -0.1466, -0.3794, -0.5147,  0.6037, -0.1632,\n",
      "          -0.2019, -0.2621,  0.0792, -0.1555, -0.6201,  0.1968,  0.2395,\n",
      "           0.3290,  0.3209, -0.1289, -0.0145,  0.0753, -0.3917, -0.0347,\n",
      "          -0.1964, -0.0207, -0.6257,  0.1745,  0.2779,  0.1662,  0.1325,\n",
      "          -0.4260, -0.1690,  0.0456,  0.0269, -0.4796, -0.3417,  0.5181,\n",
      "           0.2586, -0.0768,  0.2275,  0.1822, -0.2056, -0.3315,  0.1315,\n",
      "           0.4776,  0.5130, -0.0749,  0.2943, -0.1481,  0.4000,  0.2555,\n",
      "          -0.1629,  0.1340,  0.2801, -0.0720,  0.0179, -0.1649,  0.1585,\n",
      "          -0.1970, -0.2683,  0.1857, -0.0015,  0.0548, -0.1165, -0.2884,\n",
      "           0.3706, -0.2180,  0.0587, -0.3699, -0.2207,  0.2692,  0.3420,\n",
      "          -0.5798, -0.2572, -0.3127, -0.2968, -0.0522, -0.0697,  0.1842,\n",
      "          -0.1787,  0.1028, -0.3503,  0.1981,  0.2490, -0.0686, -0.3385,\n",
      "          -0.0743, -0.1536, -0.4746,  0.1151, -0.1358,  0.0134,  0.4935,\n",
      "           0.0255, -0.0435,  0.0146, -0.0171,  0.4015,  0.0139, -0.4152,\n",
      "          -0.1732,  0.1747,  0.1138, -0.5506, -0.4061, -0.0596,  0.0135,\n",
      "          -0.0058,  0.4120, -0.4948,  0.2071,  0.1364,  0.3497, -0.0270,\n",
      "           0.2400,  0.3586, -0.5924,  0.0396, -0.2836, -0.4379,  0.5849,\n",
      "          -0.1250, -0.4920,  0.0408, -0.2045,  0.1314, -0.4093, -0.0511,\n",
      "           0.1014, -0.1091,  0.3685,  0.2861,  0.1049,  0.2294, -0.0970,\n",
      "           0.3303, -0.1756, -0.4666,  0.0764, -0.2008,  0.1641,  0.5250,\n",
      "          -0.1371, -0.1441,  0.9853, -0.4288, -0.1299,  0.3128, -0.0875,\n",
      "          -0.5849,  0.2142, -0.0483,  0.2239, -0.0437,  0.3949, -0.2786,\n",
      "           0.2344,  0.1405, -0.0525, -0.1127, -0.3531,  0.0838,  0.5194,\n",
      "          -0.2211, -0.1347,  0.1460, -0.1113],\n",
      "         [ 0.1886,  0.1072, -0.0330, -0.4197, -0.3136,  0.4030,  0.3946,\n",
      "          -0.4305,  0.0640, -0.1055, -0.2945, -0.1145,  0.0929, -0.5379,\n",
      "           0.1828,  0.0080,  0.0162, -0.2035, -0.0881, -0.1459,  0.1725,\n",
      "           0.3021, -0.2431,  0.1309, -0.2602,  0.1552, -0.2263, -0.1244,\n",
      "          -0.5141,  0.0879, -0.3469, -0.2744, -0.0243,  0.4979, -0.0830,\n",
      "           0.3560,  0.4779,  0.0590,  0.2142,  0.0198,  0.0920,  0.2756,\n",
      "           0.0779, -0.0605,  0.0339,  0.0313,  0.0100, -0.3175, -0.1855,\n",
      "          -0.0414,  0.1156,  0.0641, -0.2488, -0.1618, -0.0617,  0.1850,\n",
      "          -0.6270, -0.4162, -0.3792, -0.1161,  0.5552, -0.1752, -0.1203,\n",
      "          -0.3961, -0.3478, -0.2329, -0.3305, -0.0372,  0.0411, -0.1485,\n",
      "           0.1871,  0.4774, -0.1943,  0.2377, -0.1686,  0.0592,  0.5432,\n",
      "           0.0780, -0.3786, -0.0435,  0.0697,  0.1386, -0.4839, -0.0526,\n",
      "          -0.3379,  0.1894, -0.7981, -0.3734,  0.1501,  0.1081,  0.7267,\n",
      "          -0.3596,  0.0451, -0.0390, -0.1951, -0.7722, -0.1456,  0.3783,\n",
      "          -0.5824, -0.3049, -0.4979,  0.1583, -0.1240,  0.0549,  0.1376,\n",
      "          -0.1595, -0.5180,  0.0791, -0.1447,  0.2102, -0.3151,  0.1099,\n",
      "           0.3974,  0.0652, -0.2175,  0.2648, -0.2079, -0.3155, -0.1125,\n",
      "           0.2393,  0.0285, -0.1090,  0.3362,  0.1495,  0.1382, -0.3636,\n",
      "           0.0378,  0.0301, -0.4862, -0.3466,  0.1131,  0.1475, -0.1413,\n",
      "           0.1392,  0.2320, -0.0775,  0.3869, -0.0412,  0.1701,  0.0067,\n",
      "          -0.0487,  0.5245,  0.5177, -0.1310, -0.2029, -0.3842, -0.1470,\n",
      "          -0.3135, -0.4436, -0.4197,  0.2409, -0.0953,  0.0398,  0.2084,\n",
      "           0.0114,  0.0442, -0.0416,  0.0876,  0.4446, -0.3209, -0.2734,\n",
      "          -0.1191,  0.1807, -0.1940,  0.3180, -0.2469,  0.3327,  0.0667,\n",
      "           0.0840,  0.4417,  0.2344,  0.2284,  0.3474,  0.1680,  0.0262,\n",
      "          -0.1952,  0.3349, -0.4517, -0.3526,  0.0859,  0.3239,  0.2955,\n",
      "           0.2638, -0.0148,  0.1795, -0.1793,  0.1798, -0.0690,  0.1676,\n",
      "          -0.3618,  0.0387, -0.5093,  0.3336, -0.0971, -0.4867, -0.2655,\n",
      "          -0.2668,  0.2094, -0.2217,  0.1134,  0.2543, -0.2130,  0.0463,\n",
      "          -0.3767, -0.2964, -0.1136,  0.4052,  0.2125, -0.4053,  0.1870,\n",
      "           0.3943,  0.0903,  0.2409, -0.3378,  0.0794, -0.5044,  0.1758,\n",
      "          -0.3333, -0.4242, -0.1152, -0.1065,  0.3474, -0.0794,  0.3216,\n",
      "          -0.3435, -0.1904, -0.2817,  0.1593,  0.5548, -0.3496, -0.0811,\n",
      "           0.3488, -0.1006,  0.3425,  0.0084, -0.0292, -0.3878,  0.1795,\n",
      "           0.2077, -0.2465,  0.3514,  0.0402, -0.7042,  0.2174, -0.7204,\n",
      "           0.2947,  0.1444,  0.3196, -0.1608,  0.4951, -0.3165,  0.6028,\n",
      "           0.2949,  0.2376, -0.1500,  0.2078],\n",
      "         [-0.0196, -0.0487, -0.0480,  0.0970,  0.0366,  0.2949,  0.2638,\n",
      "          -0.6300, -0.1567,  0.0265, -0.0902,  0.1045, -0.2607,  0.2197,\n",
      "           0.3041, -0.2275,  0.3856, -0.0828,  0.1638, -0.1172, -0.0383,\n",
      "           0.3406,  0.1846,  0.1427, -0.4139, -0.2767, -0.0462, -0.0773,\n",
      "           0.3538, -0.1389, -0.2503,  0.3082, -0.0642, -0.2628,  0.0390,\n",
      "          -0.2813, -0.0389,  0.0629, -0.0701, -0.1229,  0.7014, -0.1050,\n",
      "          -0.2981, -0.2936, -0.2363, -0.2459, -0.0340,  0.3137, -0.3107,\n",
      "          -0.0924, -0.3121,  0.0243, -0.0637,  0.0785,  0.1235, -0.0819,\n",
      "          -0.4449,  0.1467,  0.0266,  0.1221,  0.3630,  0.1167,  0.4478,\n",
      "          -0.3880, -0.3069,  0.2207,  0.0426,  0.1536,  0.0719,  0.1731,\n",
      "          -0.0119, -0.1885, -0.2160,  0.0089,  0.2400,  0.6092, -0.1362,\n",
      "          -0.1193, -0.0469,  0.1887,  0.1908, -0.3896, -0.0879,  0.0992,\n",
      "          -0.0440, -0.1344, -0.0690,  0.1123, -0.0713, -0.1686, -0.0438,\n",
      "          -0.3186,  0.1223, -0.1911, -0.0524,  0.5554,  0.4164, -0.1996,\n",
      "           0.0126,  0.3404,  0.2089,  0.1988, -0.1215, -0.1098, -0.2639,\n",
      "          -0.4574,  0.2045, -0.2022, -0.5103,  0.4486,  0.1157,  0.0100,\n",
      "          -0.2151,  0.0071, -0.0818, -0.2215, -0.3555,  0.1384,  0.4409,\n",
      "           0.4144,  0.1296, -0.4208,  0.3591, -0.0330, -0.2953, -0.1521,\n",
      "           0.0605,  0.1064, -0.2199,  0.1786,  0.3090,  0.5278,  0.0413,\n",
      "          -0.3729,  0.2015,  0.3576,  0.3533, -0.0448, -0.6589, -0.1168,\n",
      "          -0.4212,  0.0175, -0.0406, -0.4376, -0.1342, -0.7642, -0.3138,\n",
      "          -0.0425,  0.2275, -0.2316,  0.4299,  0.3517, -0.4136,  0.4420,\n",
      "          -0.3376, -0.1381,  0.3026,  0.2260,  0.1829,  0.0570, -0.2886,\n",
      "           0.0251,  0.3079, -0.5398, -0.3611,  0.2500, -0.1558, -0.5559,\n",
      "           0.3425,  0.0894, -0.0596,  0.0168, -0.2304,  0.0105, -0.3168,\n",
      "          -0.1823,  0.0559,  0.2117, -0.0810, -0.0944, -0.5354,  0.1515,\n",
      "          -0.0483,  0.1899,  0.1205,  0.1056,  0.0924, -0.0986,  0.0379,\n",
      "           0.1601,  0.1645,  0.2798, -0.0481,  0.3184,  0.1756,  0.0897,\n",
      "           0.0590, -0.0694,  0.1492,  0.3779, -0.0677,  0.0526,  0.0373,\n",
      "          -0.3241, -0.3060, -0.0197,  0.1357,  0.2163,  0.4173, -0.3143,\n",
      "           0.2590,  0.0135, -0.1187,  0.0343,  0.4154, -0.4089,  0.0647,\n",
      "          -0.3940,  0.0870, -0.1605, -0.1472, -0.2772, -0.3107, -0.7292,\n",
      "           0.3527, -0.0968,  0.4059,  0.0302, -0.1670, -0.4606,  0.0925,\n",
      "           0.1245,  0.0826,  0.2120, -0.1322, -0.1289, -0.0359,  0.3564,\n",
      "          -0.0569, -0.0845,  0.1382,  0.3513,  0.0441, -0.1983, -0.1204,\n",
      "           0.1107,  0.1301, -0.4547, -0.0819,  0.5940,  0.2460,  0.2714,\n",
      "           0.0119,  0.3590, -0.1077,  0.2032]]], grad_fn=<StackBackward0>)))\n",
      "(tensor([[[ 0.1772, -0.0030, -0.0175,  ...,  0.1353,  0.1459,  0.1643]],\n",
      "\n",
      "        [[ 0.1233,  0.0928, -0.0409,  ...,  0.2763,  0.0045,  0.1666]],\n",
      "\n",
      "        [[ 0.0977,  0.0976, -0.1538,  ...,  0.1217, -0.0644,  0.1592]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0888, -0.0585, -0.2366,  ..., -0.1940,  0.0435, -0.1709]],\n",
      "\n",
      "        [[-0.0066,  0.0038, -0.0483,  ..., -0.0594,  0.1270, -0.1004]],\n",
      "\n",
      "        [[-0.1035, -0.0160,  0.0281,  ..., -0.0777,  0.0550, -0.0600]]],\n",
      "       grad_fn=<StackBackward0>), (tensor([[[-1.0350e-01, -1.6034e-02,  2.8122e-02, -1.8325e-02,  2.3790e-01,\n",
      "           4.3031e-02,  1.8411e-02, -5.0801e-02,  1.3055e-01, -6.3131e-02,\n",
      "           6.7453e-02,  5.9371e-02,  1.4911e-01, -2.2793e-01,  5.7851e-02,\n",
      "          -1.0279e-01, -1.5936e-01,  1.1123e-01,  1.8692e-02, -3.4645e-02,\n",
      "           5.1580e-02,  2.2363e-01, -2.7622e-02,  1.1587e-01, -1.5806e-01,\n",
      "           1.6889e-01, -4.2855e-02,  1.6434e-02, -2.0959e-01, -1.8929e-01,\n",
      "           1.9799e-01,  1.4755e-01, -1.2166e-01,  4.0538e-02,  5.2644e-02,\n",
      "           1.7087e-01,  2.5952e-01,  1.8656e-02, -8.3799e-02, -2.3422e-02,\n",
      "          -3.4853e-04,  1.3201e-01,  3.3242e-02,  5.5901e-02,  2.1833e-01,\n",
      "           1.7916e-01,  1.6410e-01, -2.3803e-02, -4.1723e-02,  7.0927e-02,\n",
      "          -1.0883e-01,  1.5670e-01, -1.2340e-01, -1.4682e-01,  4.8165e-02,\n",
      "           2.8679e-01,  5.0911e-02, -3.5721e-01,  6.2375e-04, -1.3237e-02,\n",
      "          -1.2429e-01, -1.1694e-01, -3.0828e-02, -1.5222e-01,  4.8957e-02,\n",
      "          -9.3080e-02,  2.0189e-01,  1.6854e-01,  1.1730e-01,  5.5127e-02,\n",
      "          -2.0660e-02,  1.7010e-01,  2.4232e-02,  1.4064e-02, -1.7917e-02,\n",
      "           1.7411e-01,  8.3744e-02,  2.2516e-01, -1.4824e-01,  4.1049e-02,\n",
      "           1.0172e-01,  9.0689e-02,  5.8674e-02, -2.1258e-01, -1.1074e-01,\n",
      "           2.0143e-01, -1.7162e-01, -3.9319e-02,  1.4156e-01, -1.6870e-01,\n",
      "           3.7583e-01, -4.3432e-02, -1.1666e-01, -1.0779e-01, -2.8401e-01,\n",
      "          -3.0816e-01,  2.7313e-01, -7.4010e-02, -5.4276e-02, -1.1928e-01,\n",
      "           4.3603e-02, -7.1930e-02, -2.7624e-01,  8.7755e-02,  8.7431e-02,\n",
      "           1.9529e-01,  2.2817e-01, -1.0347e-01, -9.0389e-03,  3.9333e-02,\n",
      "          -1.4451e-01, -2.3187e-02, -1.4143e-01, -1.4927e-02, -3.5430e-01,\n",
      "           1.4252e-01,  6.3867e-02,  8.9375e-02,  8.8929e-02, -1.7010e-01,\n",
      "          -6.4698e-02,  3.0194e-02,  8.0792e-03, -2.3439e-01, -6.4244e-02,\n",
      "           2.4263e-01,  1.3398e-01, -3.2969e-02,  1.0408e-01,  6.4239e-02,\n",
      "          -1.4487e-01, -2.2330e-01,  7.8249e-02,  3.1752e-01,  2.3070e-01,\n",
      "          -3.9203e-02,  9.4702e-02, -8.8557e-02,  2.2314e-01,  1.5419e-01,\n",
      "          -1.1132e-01,  2.4215e-02,  1.4055e-01, -3.7196e-02,  1.0809e-02,\n",
      "          -8.4072e-02,  9.3972e-02, -9.0882e-02, -1.4849e-01,  1.0084e-01,\n",
      "          -8.5263e-04,  3.4429e-02, -5.2846e-02, -1.7037e-01,  1.8789e-01,\n",
      "          -5.9357e-02,  1.5746e-02, -2.3366e-01, -1.5807e-01,  9.5178e-02,\n",
      "           1.6516e-01, -2.1302e-01, -1.3769e-01, -6.4045e-02, -1.3557e-01,\n",
      "          -3.1246e-02, -3.4470e-02,  1.2257e-01, -7.3063e-02,  4.1710e-02,\n",
      "          -2.1955e-01,  7.1316e-02,  8.8944e-02, -2.2196e-02, -1.9711e-01,\n",
      "          -5.5013e-02, -7.9094e-02, -2.6078e-01,  6.6477e-02, -5.9758e-02,\n",
      "           4.9942e-03,  2.7293e-01,  9.9934e-03, -1.7192e-02,  9.5103e-03,\n",
      "          -1.0415e-02,  1.5063e-01,  3.9423e-03, -2.5347e-01, -6.9253e-02,\n",
      "           9.6336e-02,  5.8478e-02, -2.2049e-01, -1.3453e-01, -3.3551e-02,\n",
      "           7.6300e-03, -2.5498e-03,  1.9828e-01, -2.2724e-01,  1.0243e-01,\n",
      "           4.9353e-02,  1.3062e-01, -1.0231e-02,  1.1317e-01,  1.8400e-01,\n",
      "          -4.0209e-01,  2.2878e-02, -1.3901e-01, -2.8087e-01,  2.7705e-01,\n",
      "          -8.1692e-02, -2.7200e-01,  2.2633e-02, -9.7501e-02,  9.6411e-02,\n",
      "          -1.7014e-01, -4.0242e-02,  5.8845e-02, -6.7816e-02,  1.9617e-01,\n",
      "           1.4886e-01,  4.7680e-02,  7.3001e-02, -5.6526e-02,  1.3770e-01,\n",
      "          -1.0867e-01, -2.0852e-01,  4.2697e-02, -8.7231e-02,  6.5406e-02,\n",
      "           2.2227e-01, -8.5916e-02, -5.1824e-02,  4.2150e-01, -2.3031e-01,\n",
      "          -7.0769e-02,  2.2283e-01, -4.0743e-02, -2.1551e-01,  1.0560e-01,\n",
      "          -2.4911e-02,  4.9446e-02, -1.8352e-02,  2.5352e-01, -1.5141e-01,\n",
      "           8.9892e-02,  6.5212e-02, -3.4348e-02, -4.0323e-02, -1.9325e-01,\n",
      "           5.3401e-02,  2.5423e-01, -1.0113e-01, -7.7651e-02,  5.4962e-02,\n",
      "          -6.0011e-02]]], grad_fn=<StackBackward0>), tensor([[[-0.2378, -0.0686,  0.0504, -0.0386,  0.4264,  0.0833,  0.0982,\n",
      "          -0.2152,  0.2842, -0.0915,  0.1192,  0.1679,  0.2215, -0.3277,\n",
      "           0.1604, -0.1487, -0.3572,  0.2746,  0.0438, -0.0961,  0.1699,\n",
      "           0.6211, -0.0542,  0.1776, -0.4394,  0.3717, -0.0959,  0.0891,\n",
      "          -0.4313, -0.5618,  0.4027,  0.2785, -0.2535,  0.0840,  0.1270,\n",
      "           0.3027,  0.7668,  0.0322, -0.2195, -0.0436, -0.0012,  0.2164,\n",
      "           0.0704,  0.1454,  0.4501,  0.6147,  0.2265, -0.0530, -0.0747,\n",
      "           0.1120, -0.1657,  0.3025, -0.3325, -0.3433,  0.1258,  0.4463,\n",
      "           0.0996, -0.6229,  0.0018, -0.0356, -0.2726, -0.2417, -0.0918,\n",
      "          -0.4502,  0.0853, -0.1656,  0.2970,  0.2218,  0.2707,  0.1433,\n",
      "          -0.0417,  0.3122,  0.0585,  0.0283, -0.0339,  0.2874,  0.1564,\n",
      "           0.5010, -0.2773,  0.0578,  0.1477,  0.2291,  0.1401, -0.3924,\n",
      "          -0.2388,  0.3224, -0.3023, -0.0951,  0.1753, -0.2752,  0.7025,\n",
      "          -0.1467, -0.3427, -0.1466, -0.3794, -0.5147,  0.6037, -0.1632,\n",
      "          -0.2019, -0.2621,  0.0792, -0.1555, -0.6201,  0.1968,  0.2395,\n",
      "           0.3290,  0.3209, -0.1289, -0.0145,  0.0753, -0.3917, -0.0347,\n",
      "          -0.1964, -0.0207, -0.6257,  0.1745,  0.2779,  0.1662,  0.1325,\n",
      "          -0.4260, -0.1690,  0.0456,  0.0269, -0.4796, -0.3417,  0.5181,\n",
      "           0.2586, -0.0768,  0.2275,  0.1822, -0.2056, -0.3315,  0.1315,\n",
      "           0.4776,  0.5130, -0.0749,  0.2943, -0.1481,  0.4000,  0.2555,\n",
      "          -0.1629,  0.1340,  0.2801, -0.0720,  0.0179, -0.1649,  0.1585,\n",
      "          -0.1970, -0.2683,  0.1857, -0.0015,  0.0548, -0.1165, -0.2884,\n",
      "           0.3706, -0.2180,  0.0587, -0.3699, -0.2207,  0.2692,  0.3420,\n",
      "          -0.5798, -0.2572, -0.3127, -0.2968, -0.0522, -0.0697,  0.1842,\n",
      "          -0.1787,  0.1028, -0.3503,  0.1981,  0.2490, -0.0686, -0.3385,\n",
      "          -0.0743, -0.1536, -0.4746,  0.1151, -0.1358,  0.0134,  0.4935,\n",
      "           0.0255, -0.0435,  0.0146, -0.0171,  0.4015,  0.0139, -0.4152,\n",
      "          -0.1732,  0.1747,  0.1138, -0.5506, -0.4061, -0.0596,  0.0135,\n",
      "          -0.0058,  0.4120, -0.4948,  0.2071,  0.1364,  0.3497, -0.0270,\n",
      "           0.2400,  0.3586, -0.5924,  0.0396, -0.2836, -0.4379,  0.5849,\n",
      "          -0.1250, -0.4920,  0.0408, -0.2045,  0.1314, -0.4093, -0.0511,\n",
      "           0.1014, -0.1091,  0.3685,  0.2861,  0.1049,  0.2294, -0.0970,\n",
      "           0.3303, -0.1756, -0.4666,  0.0764, -0.2008,  0.1641,  0.5250,\n",
      "          -0.1371, -0.1441,  0.9853, -0.4288, -0.1299,  0.3128, -0.0875,\n",
      "          -0.5849,  0.2142, -0.0483,  0.2239, -0.0437,  0.3949, -0.2786,\n",
      "           0.2344,  0.1405, -0.0525, -0.1127, -0.3531,  0.0838,  0.5194,\n",
      "          -0.2211, -0.1347,  0.1460, -0.1113]]], grad_fn=<StackBackward0>)))\n",
      "(tensor([[[ 0.0539,  0.1812, -0.0987,  ..., -0.1696, -0.0965,  0.0914]],\n",
      "\n",
      "        [[-0.1003,  0.1147, -0.1272,  ..., -0.0991, -0.1116,  0.0089]],\n",
      "\n",
      "        [[-0.0858,  0.0395,  0.1421,  ..., -0.1521, -0.0932,  0.1047]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1558,  0.1998, -0.1389,  ...,  0.1352, -0.0659, -0.1239]],\n",
      "\n",
      "        [[ 0.1518,  0.2188, -0.1322,  ..., -0.0572,  0.0911,  0.0628]],\n",
      "\n",
      "        [[ 0.0824,  0.0562, -0.0170,  ...,  0.1234, -0.0767,  0.0816]]],\n",
      "       grad_fn=<StackBackward0>), (tensor([[[ 0.0824,  0.0562, -0.0170, -0.2061, -0.1504,  0.0759,  0.1793,\n",
      "          -0.2028,  0.0276, -0.0696, -0.1209, -0.0613,  0.0474, -0.2472,\n",
      "           0.0833,  0.0018,  0.0045, -0.0767, -0.0397, -0.0855,  0.0896,\n",
      "           0.1487, -0.1002,  0.0753, -0.1212,  0.0907, -0.1101, -0.0467,\n",
      "          -0.2885,  0.0555, -0.1793, -0.0632, -0.0122,  0.2576, -0.0368,\n",
      "           0.1588,  0.1160,  0.0261,  0.1005,  0.0127,  0.0521,  0.1204,\n",
      "           0.0368, -0.0393,  0.0184,  0.0173,  0.0043, -0.0867, -0.1229,\n",
      "          -0.0148,  0.0180,  0.0344, -0.1293, -0.0763, -0.0303,  0.0768,\n",
      "          -0.2813, -0.1977, -0.2286, -0.0637,  0.2213, -0.0594, -0.0743,\n",
      "          -0.2494, -0.2662, -0.1900, -0.1108, -0.0206,  0.0193, -0.0688,\n",
      "           0.1078,  0.3102, -0.1085,  0.1086, -0.1271,  0.0357,  0.2170,\n",
      "           0.0476, -0.1983, -0.0167,  0.0318,  0.0408, -0.2625, -0.0386,\n",
      "          -0.1041,  0.0735, -0.4395, -0.1947,  0.0922,  0.0445,  0.2970,\n",
      "          -0.1065,  0.0229, -0.0085, -0.0451, -0.1466, -0.0707,  0.0882,\n",
      "          -0.1223, -0.1776, -0.2822,  0.0571, -0.0740,  0.0244,  0.0813,\n",
      "          -0.0684, -0.1809,  0.0250, -0.0386,  0.0671, -0.1412,  0.0499,\n",
      "           0.1385,  0.0469, -0.1228,  0.0950, -0.1295, -0.1496, -0.0601,\n",
      "           0.1370,  0.0120, -0.0560,  0.1911,  0.0659,  0.0523, -0.1329,\n",
      "           0.0146,  0.0189, -0.2003, -0.1746,  0.0439,  0.0790, -0.0510,\n",
      "           0.1002,  0.1068, -0.0405,  0.2071, -0.0150,  0.1126,  0.0019,\n",
      "          -0.0180,  0.1851,  0.2046, -0.0831, -0.1545, -0.2487, -0.0489,\n",
      "          -0.1872, -0.2262, -0.2685,  0.0507, -0.0397,  0.0119,  0.1162,\n",
      "           0.0063,  0.0173, -0.0091,  0.0419,  0.2489, -0.2038, -0.1041,\n",
      "          -0.0484,  0.0915, -0.0746,  0.1118, -0.1192,  0.1110,  0.0262,\n",
      "           0.0507,  0.2466,  0.1263,  0.0765,  0.1367,  0.1239,  0.0119,\n",
      "          -0.0773,  0.2201, -0.2023, -0.0817,  0.0463,  0.1285,  0.0823,\n",
      "           0.1837, -0.0076,  0.0913, -0.1199,  0.0675, -0.0379,  0.0794,\n",
      "          -0.1108,  0.0192, -0.1759,  0.1715, -0.0549, -0.1352, -0.1064,\n",
      "          -0.1121,  0.0883, -0.0962,  0.0663,  0.0614, -0.0983,  0.0342,\n",
      "          -0.2035, -0.1700, -0.0551,  0.1698,  0.1033, -0.2122,  0.0738,\n",
      "           0.1479,  0.0283,  0.1065, -0.1125,  0.0511, -0.3024,  0.0865,\n",
      "          -0.1693, -0.2857, -0.0677, -0.0764,  0.1804, -0.0287,  0.1511,\n",
      "          -0.2456, -0.1163, -0.1508,  0.0710,  0.3653, -0.1652, -0.0483,\n",
      "           0.1885, -0.0351,  0.1499,  0.0049, -0.0192, -0.1867,  0.0745,\n",
      "           0.0478, -0.1231,  0.1231,  0.0193, -0.2663,  0.1544, -0.3307,\n",
      "           0.1182,  0.1017,  0.1472, -0.0946,  0.1166, -0.1880,  0.2998,\n",
      "           0.1102,  0.1234, -0.0767,  0.0816]]], grad_fn=<StackBackward0>), tensor([[[ 0.1886,  0.1072, -0.0330, -0.4197, -0.3136,  0.4030,  0.3946,\n",
      "          -0.4305,  0.0640, -0.1055, -0.2945, -0.1145,  0.0929, -0.5379,\n",
      "           0.1828,  0.0080,  0.0162, -0.2035, -0.0881, -0.1459,  0.1725,\n",
      "           0.3021, -0.2431,  0.1309, -0.2602,  0.1552, -0.2263, -0.1244,\n",
      "          -0.5141,  0.0879, -0.3469, -0.2744, -0.0243,  0.4979, -0.0830,\n",
      "           0.3560,  0.4779,  0.0590,  0.2142,  0.0198,  0.0920,  0.2756,\n",
      "           0.0779, -0.0605,  0.0339,  0.0313,  0.0100, -0.3175, -0.1855,\n",
      "          -0.0414,  0.1156,  0.0641, -0.2488, -0.1618, -0.0617,  0.1850,\n",
      "          -0.6270, -0.4162, -0.3792, -0.1161,  0.5552, -0.1752, -0.1203,\n",
      "          -0.3961, -0.3478, -0.2329, -0.3305, -0.0372,  0.0411, -0.1485,\n",
      "           0.1871,  0.4774, -0.1943,  0.2377, -0.1686,  0.0592,  0.5432,\n",
      "           0.0780, -0.3786, -0.0435,  0.0697,  0.1386, -0.4839, -0.0526,\n",
      "          -0.3379,  0.1894, -0.7981, -0.3734,  0.1501,  0.1081,  0.7267,\n",
      "          -0.3596,  0.0451, -0.0390, -0.1951, -0.7722, -0.1456,  0.3783,\n",
      "          -0.5824, -0.3049, -0.4979,  0.1583, -0.1240,  0.0549,  0.1376,\n",
      "          -0.1595, -0.5180,  0.0791, -0.1447,  0.2102, -0.3151,  0.1099,\n",
      "           0.3974,  0.0652, -0.2175,  0.2648, -0.2079, -0.3155, -0.1125,\n",
      "           0.2393,  0.0285, -0.1090,  0.3362,  0.1495,  0.1382, -0.3636,\n",
      "           0.0378,  0.0301, -0.4862, -0.3466,  0.1131,  0.1475, -0.1413,\n",
      "           0.1392,  0.2320, -0.0775,  0.3869, -0.0412,  0.1701,  0.0067,\n",
      "          -0.0487,  0.5245,  0.5177, -0.1310, -0.2029, -0.3842, -0.1470,\n",
      "          -0.3135, -0.4436, -0.4197,  0.2409, -0.0953,  0.0398,  0.2084,\n",
      "           0.0114,  0.0442, -0.0416,  0.0876,  0.4446, -0.3209, -0.2734,\n",
      "          -0.1191,  0.1807, -0.1940,  0.3180, -0.2469,  0.3327,  0.0667,\n",
      "           0.0840,  0.4417,  0.2344,  0.2284,  0.3474,  0.1680,  0.0262,\n",
      "          -0.1952,  0.3349, -0.4517, -0.3526,  0.0859,  0.3239,  0.2955,\n",
      "           0.2638, -0.0148,  0.1795, -0.1793,  0.1798, -0.0690,  0.1676,\n",
      "          -0.3618,  0.0387, -0.5093,  0.3336, -0.0971, -0.4867, -0.2655,\n",
      "          -0.2668,  0.2094, -0.2217,  0.1134,  0.2543, -0.2130,  0.0463,\n",
      "          -0.3767, -0.2964, -0.1136,  0.4052,  0.2125, -0.4053,  0.1870,\n",
      "           0.3943,  0.0903,  0.2409, -0.3378,  0.0794, -0.5044,  0.1758,\n",
      "          -0.3333, -0.4242, -0.1152, -0.1065,  0.3474, -0.0794,  0.3216,\n",
      "          -0.3435, -0.1904, -0.2817,  0.1593,  0.5548, -0.3496, -0.0811,\n",
      "           0.3488, -0.1006,  0.3425,  0.0084, -0.0292, -0.3878,  0.1795,\n",
      "           0.2077, -0.2465,  0.3514,  0.0402, -0.7042,  0.2174, -0.7204,\n",
      "           0.2947,  0.1444,  0.3196, -0.1608,  0.4951, -0.3165,  0.6028,\n",
      "           0.2949,  0.2376, -0.1500,  0.2078]]], grad_fn=<StackBackward0>)))\n",
      "(tensor([[[ 0.0516,  0.1224, -0.2111,  ..., -0.1324, -0.0516, -0.1286]],\n",
      "\n",
      "        [[ 0.0938,  0.2631, -0.0165,  ...,  0.0451, -0.0726, -0.0200]],\n",
      "\n",
      "        [[ 0.0137,  0.1780, -0.1056,  ...,  0.1066, -0.0932, -0.0739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1531,  0.0999,  0.1398,  ...,  0.0898,  0.1119,  0.0859]],\n",
      "\n",
      "        [[ 0.0106, -0.0979,  0.0357,  ..., -0.0018, -0.1023, -0.0023]],\n",
      "\n",
      "        [[-0.0055, -0.0293, -0.0205,  ...,  0.2341, -0.0705,  0.0527]]],\n",
      "       grad_fn=<StackBackward0>), (tensor([[[-0.0055, -0.0293, -0.0205,  0.0753,  0.0112,  0.1400,  0.0976,\n",
      "          -0.1713, -0.0630,  0.0120, -0.0640,  0.0511, -0.1409,  0.0766,\n",
      "           0.1090, -0.0706,  0.2633, -0.0541,  0.0642, -0.0567, -0.0240,\n",
      "           0.1159,  0.0781,  0.0557, -0.1089, -0.1111, -0.0230, -0.0520,\n",
      "           0.0930, -0.0912, -0.1004,  0.1540, -0.0175, -0.1538,  0.0169,\n",
      "          -0.0920, -0.0275,  0.0270, -0.0403, -0.0710,  0.3768, -0.0404,\n",
      "          -0.1134, -0.2061, -0.1046, -0.1621, -0.0251,  0.1734, -0.2493,\n",
      "          -0.0570, -0.1772,  0.0118, -0.0385,  0.0422,  0.0687, -0.0273,\n",
      "          -0.2700,  0.0410,  0.0196,  0.0425,  0.2263,  0.0614,  0.1597,\n",
      "          -0.2984, -0.1794,  0.0817,  0.0190,  0.0789,  0.0369,  0.0865,\n",
      "          -0.0063, -0.0878, -0.0911,  0.0026,  0.1085,  0.3375, -0.0302,\n",
      "          -0.0552, -0.0243,  0.1611,  0.0710, -0.1475, -0.0517,  0.0683,\n",
      "          -0.0241, -0.0629, -0.0367,  0.0580, -0.0286, -0.0829, -0.0128,\n",
      "          -0.0809,  0.0447, -0.1422, -0.0239,  0.3817,  0.1723, -0.0967,\n",
      "           0.0049,  0.1988,  0.0761,  0.1104, -0.0504, -0.0600, -0.0718,\n",
      "          -0.2821,  0.1276, -0.0863, -0.1258,  0.1613,  0.0727,  0.0038,\n",
      "          -0.1094,  0.0032, -0.0334, -0.1109, -0.1942,  0.0997,  0.1621,\n",
      "           0.2098,  0.0452, -0.1112,  0.0739, -0.0131, -0.1064, -0.0793,\n",
      "           0.0216,  0.0568, -0.1163,  0.0871,  0.1135,  0.2951,  0.0139,\n",
      "          -0.1212,  0.0671,  0.2445,  0.1931, -0.0202, -0.3935, -0.0763,\n",
      "          -0.2030,  0.0077, -0.0258, -0.2460, -0.0801, -0.4031, -0.1240,\n",
      "          -0.0151,  0.1339, -0.0987,  0.1580,  0.1118, -0.1476,  0.2091,\n",
      "          -0.2085, -0.0465,  0.1217,  0.1438,  0.0994,  0.0413, -0.1040,\n",
      "           0.0096,  0.1558, -0.1009, -0.2020,  0.1516, -0.0918, -0.2110,\n",
      "           0.1793,  0.0310, -0.0293,  0.0065, -0.1119,  0.0066, -0.1528,\n",
      "          -0.0750,  0.0344,  0.1084, -0.0502, -0.0491, -0.3045,  0.1060,\n",
      "          -0.0316,  0.0744,  0.0531,  0.0346,  0.0376, -0.0753,  0.0255,\n",
      "           0.0708,  0.0905,  0.0817, -0.0211,  0.1637,  0.0770,  0.0325,\n",
      "           0.0361, -0.0355,  0.1001,  0.2027, -0.0354,  0.0250,  0.0263,\n",
      "          -0.1401, -0.1812, -0.0106,  0.0640,  0.0727,  0.2190, -0.1753,\n",
      "           0.0913,  0.0037, -0.0577,  0.0203,  0.1713, -0.1004,  0.0340,\n",
      "          -0.2161,  0.0416, -0.0692, -0.1189, -0.1209, -0.1661, -0.3531,\n",
      "           0.1664, -0.0517,  0.0984,  0.0206, -0.0744, -0.1180,  0.0699,\n",
      "           0.0792,  0.0487,  0.1048, -0.0813, -0.0487, -0.0150,  0.1673,\n",
      "          -0.0238, -0.0375,  0.0585,  0.1816,  0.0160, -0.1287, -0.0547,\n",
      "           0.0280,  0.0642, -0.1853, -0.0432,  0.4148,  0.1394,  0.1001,\n",
      "           0.0032,  0.2341, -0.0705,  0.0527]]], grad_fn=<StackBackward0>), tensor([[[-0.0196, -0.0487, -0.0480,  0.0970,  0.0366,  0.2949,  0.2638,\n",
      "          -0.6300, -0.1567,  0.0265, -0.0902,  0.1045, -0.2607,  0.2197,\n",
      "           0.3041, -0.2275,  0.3856, -0.0828,  0.1638, -0.1172, -0.0383,\n",
      "           0.3406,  0.1846,  0.1427, -0.4139, -0.2767, -0.0462, -0.0773,\n",
      "           0.3538, -0.1389, -0.2503,  0.3082, -0.0642, -0.2628,  0.0390,\n",
      "          -0.2813, -0.0389,  0.0629, -0.0701, -0.1229,  0.7014, -0.1050,\n",
      "          -0.2981, -0.2936, -0.2363, -0.2459, -0.0340,  0.3137, -0.3107,\n",
      "          -0.0924, -0.3121,  0.0243, -0.0637,  0.0785,  0.1235, -0.0819,\n",
      "          -0.4449,  0.1467,  0.0266,  0.1221,  0.3630,  0.1167,  0.4478,\n",
      "          -0.3880, -0.3069,  0.2207,  0.0426,  0.1536,  0.0719,  0.1731,\n",
      "          -0.0119, -0.1885, -0.2160,  0.0089,  0.2400,  0.6092, -0.1362,\n",
      "          -0.1193, -0.0469,  0.1887,  0.1908, -0.3896, -0.0879,  0.0992,\n",
      "          -0.0440, -0.1344, -0.0690,  0.1123, -0.0713, -0.1686, -0.0438,\n",
      "          -0.3186,  0.1223, -0.1911, -0.0524,  0.5554,  0.4164, -0.1996,\n",
      "           0.0126,  0.3404,  0.2089,  0.1988, -0.1215, -0.1098, -0.2639,\n",
      "          -0.4574,  0.2045, -0.2022, -0.5103,  0.4486,  0.1157,  0.0100,\n",
      "          -0.2151,  0.0071, -0.0818, -0.2215, -0.3555,  0.1384,  0.4409,\n",
      "           0.4144,  0.1296, -0.4208,  0.3591, -0.0330, -0.2953, -0.1521,\n",
      "           0.0605,  0.1064, -0.2199,  0.1786,  0.3090,  0.5278,  0.0413,\n",
      "          -0.3729,  0.2015,  0.3576,  0.3533, -0.0448, -0.6589, -0.1168,\n",
      "          -0.4212,  0.0175, -0.0406, -0.4376, -0.1342, -0.7642, -0.3138,\n",
      "          -0.0425,  0.2275, -0.2316,  0.4299,  0.3517, -0.4136,  0.4420,\n",
      "          -0.3376, -0.1381,  0.3026,  0.2260,  0.1829,  0.0570, -0.2886,\n",
      "           0.0251,  0.3079, -0.5398, -0.3611,  0.2500, -0.1558, -0.5559,\n",
      "           0.3425,  0.0894, -0.0596,  0.0168, -0.2304,  0.0105, -0.3168,\n",
      "          -0.1823,  0.0559,  0.2117, -0.0810, -0.0944, -0.5354,  0.1515,\n",
      "          -0.0483,  0.1899,  0.1205,  0.1056,  0.0924, -0.0986,  0.0379,\n",
      "           0.1601,  0.1645,  0.2798, -0.0481,  0.3184,  0.1756,  0.0897,\n",
      "           0.0590, -0.0694,  0.1492,  0.3779, -0.0677,  0.0526,  0.0373,\n",
      "          -0.3241, -0.3060, -0.0197,  0.1357,  0.2163,  0.4173, -0.3143,\n",
      "           0.2590,  0.0135, -0.1187,  0.0343,  0.4154, -0.4089,  0.0647,\n",
      "          -0.3940,  0.0870, -0.1605, -0.1472, -0.2772, -0.3107, -0.7292,\n",
      "           0.3527, -0.0968,  0.4059,  0.0302, -0.1670, -0.4606,  0.0925,\n",
      "           0.1245,  0.0826,  0.2120, -0.1322, -0.1289, -0.0359,  0.3564,\n",
      "          -0.0569, -0.0845,  0.1382,  0.3513,  0.0441, -0.1983, -0.1204,\n",
      "           0.1107,  0.1301, -0.4547, -0.0819,  0.5940,  0.2460,  0.2714,\n",
      "           0.0119,  0.3590, -0.1077,  0.2032]]], grad_fn=<StackBackward0>)))\n",
      "All output tests are passed :)!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.rnn.tests import classifier_test, parameter_test\n",
    "from exercise_code.rnn.text_classifiers import RNNClassifier\n",
    "\n",
    "model = None\n",
    "\n",
    "########################################################################\n",
    "# TODO - Create a Model                                               #\n",
    "########################################################################\n",
    "model = RNNClassifier(\n",
    "    num_embeddings=len(vocab), \n",
    "    embedding_dim=256, \n",
    "    hidden_size=256, \n",
    "    use_lstm=True)\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "# Check whether your model is sufficiently small and have a correct output format\n",
    "parameter_test(model), classifier_test(model, len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your own model\n",
    "\n",
    "In this section, you need to train the classifier you created. Below, you can see some setup code we provided to you. Note the **collate function** used with the `DataLoader`. If you forgot why we need the collate function here, check this out in Notebook 1.\n",
    "\n",
    "You are free to change the below configs (e.g. batch size, device setting etc.) as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configs\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using {}...\\n'.format(device))\n",
    "\n",
    "# Move model to the device we are using\n",
    "model = model.to(device)\n",
    "\n",
    "# To tackle with the exploding gradient problem, you may want to set gclip and use clip_grad_norm_\n",
    "# see the optional notebook for the explanation\n",
    "gclip = None\n",
    "\n",
    "# Dataloaders, note the collate function\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, batch_size=16, collate_fn=collate, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  val_dataset, batch_size=16, collate_fn=collate, drop_last=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>Task: Implement Training</h3>\n",
    "    <p>\n",
    "        In the below cell, you are expected to implement your training loop to train your model. You can use the training loader provided above for iterating over the data. If you want to evaluate your model periodically, you may use the validation loader provided above. You can use pure PyTorch or PyTorch Lightning.\n",
    "   </p>\n",
    "</div>\n",
    "\n",
    "**Hints :**\n",
    "* Use `torch.nn.BCELoss` as loss function\n",
    "* Revise the previous exercises if you get stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 100 / 572\n",
      "Step 200 / 572\n",
      "Step 300 / 572\n",
      "Step 400 / 572\n",
      "Step 500 / 572\n",
      "Step 100 / 196\n",
      "Epoch 0: Train acc: 0.9882 | Validation acc: 0.8292\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39m, lengths)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m loss_ \u001b[39m=\u001b[39m loss_fn(pred, label)\n\u001b[0;32m---> 17\u001b[0m loss_\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m gclip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     clip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), gclip)\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.10/site-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#                     TODO - Train Your Model                          #\n",
    "########################################################################\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=[0.9,0.999],lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "loss_fn = nn.BCELoss()\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    for i, x in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        pred = model(input, lengths).to(device)\n",
    "        loss_ = loss_fn(pred, label)\n",
    "        loss_.backward()\n",
    "        if gclip is not None:\n",
    "            clip_grad_norm_(model.parameters(), gclip)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader)\n",
    "    val_acc = compute_accuracy(model, val_loader)\n",
    "    print('Epoch {}: Train acc: {:.4f} | Validation acc: {:.4f}'.format(epoch, train_acc, val_acc))\n",
    "    \n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "As you trained a model and improved it on the validation set, you can now test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 / 386\n",
      "Step 200 / 386\n",
      "Step 300 / 386\n",
      "accuracy on test set: 0.8546869931884528\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "  test_dataset, batch_size=8, collate_fn=collate, drop_last=False\n",
    ")\n",
    "\n",
    "print(\"accuracy on test set: {}\".format(compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "\n",
    "Now that you trained a sufficiently good sentiment classifier, run the below cell and type some text to see some predictions (type exit to quit the demo). Since we used a small data, don't expect too much :).\n",
    "<div class=\"alert alert-warning\">\n",
    "<h3>Warning!</h3>\n",
    "    <p>\n",
    "        As there is a while True loop in the cell below, you can skip this one for now and run the cell under '3. Submission' first to save your model. \n",
    "   </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m w2i \u001b[39m=\u001b[39m vocab\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m text \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "from exercise_code.rnn.sentiment_dataset import tokenize\n",
    "\n",
    "text = ''\n",
    "w2i = vocab\n",
    "while True:\n",
    "    text = input()\n",
    "    if text == 'exit':\n",
    "        break\n",
    "\n",
    "    words = torch.tensor([\n",
    "        w2i.get(word, w2i['<unk>'])\n",
    "        for word in tokenize(text)\n",
    "    ]).long().view(-1, 1)  # T x B\n",
    "\n",
    "    pred = model(words).item()\n",
    "    sent = pred > 0.5\n",
    "    \n",
    "    print('Sentiment -> {}, Confidence -> {}'.format(\n",
    "        ':)' if sent else ':(', pred if sent else 1 - pred\n",
    "    ))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Submission\n",
    "\n",
    "If you got sufficient performance on the test data, you are ready to upload your model to the [server](https://i2dl.vc.in.tum.de/submission/) . As always, let's first save your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Your model is saved to models/rnn_classifier.p successfully!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/rnn_classifier.p'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.util.save_model import save_model\n",
    "\n",
    "save_model(model, 'rnn_classifier.p')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you finished the last I2DL exercise! One last time this semester, let's prepare the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['exercise_code', 'models']\n",
      "notebooks files: ['2_sentiment_analysis.ipynb', '1_text_preprocessing_and_embedding.ipynb', 'Optional-recurrent_neural_networks.ipynb']\n",
      "Adding folder exercise_code\n",
      "Adding folder models\n",
      "Adding notebook 2_sentiment_analysis.ipynb\n",
      "Adding notebook 1_text_preprocessing_and_embedding.ipynb\n",
      "Adding notebook Optional-recurrent_neural_networks.ipynb\n",
      "Zipping successful! Zip is stored under: /Users/karo/Desktop/i2dl_exercises/output/exercise11.zip\n"
     ]
    }
   ],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise11')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/) with your account details and upload the zip file.\n",
    "3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "4. Within the working period, you can submit as many solutions as you want to get the best possible score.\n",
    "\n",
    "# 5. Submission Goals\n",
    "\n",
    "- Goal: Implement and train a recurrent neural network for sentiment analysis.\n",
    "- Passing Criteria: Reach **Accuracy >= 83%** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __July 6, 2023, 10.00__\n",
    "- Submission deadline: __July 12, 2023, 15.59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+11:+RNNs)\n",
    "\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+11:+RNNs) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae3aae73068e3f6c78354faadc00aa3f23e0713f86a27300232dd83e2bc002d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
